{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clean_Emotion_Recognition_Facemesh.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "QwrDEdfmmPOG",
        "cLoxG3LbyYJl",
        "Gvy3jwtpnuEJ"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnavm30/EmotionRecognition/blob/main/Public_Emotion_Recognition_Facemesh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSQOnESMkSMO"
      },
      "source": [
        "# Installing and importing modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K2gZm_O-bqM"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNib7uX-4d5S"
      },
      "source": [
        "!pip install mediapipe pandas numpy opencv-python imbalanced-learn keras keras-tuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsNteACu3PYt"
      },
      "source": [
        "import mediapipe as mp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wuup5erM3Yey"
      },
      "source": [
        "# Creating CSV of landmarks from images in ExpW\n",
        "\n",
        "Only run the first time: csv saved to drive after."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2_q1AfSkjvu"
      },
      "source": [
        "Generating header row"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnznGgNY3Yvt"
      },
      "source": [
        "num_coords = 468\n",
        "header_row = ['expression_label']\n",
        "\n",
        "for i in range(1, num_coords+1):\n",
        "    header_row += [f'x{i}', f'y{i}', f'z{i}']\n",
        "\n",
        "with open('/home/arnav/Workarea/MentalStateProject/facemesh_landmarks.csv', mode='w', newline='') as f:\n",
        "    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    csv_writer.writerow(header_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTa0iiMpko4_"
      },
      "source": [
        "Helper function that maps image to emotion label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxxB9rvC3gs5"
      },
      "source": [
        "def lookup(image_name):\n",
        "    with open('/home/arnav/Workarea/MentalStateProject/label.lst') as f:\n",
        "        for line in f:\n",
        "            if image_name in line:\n",
        "                return line[-2] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvwcQ4PKlrs-"
      },
      "source": [
        "Generating face mesh for each static image with one face (with multiple returns None) and then adding x, y, z of all 468 landmarks to each column of csv. Also padding and resizing image to (224,224) and organizing images by label into corresponding folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNwozETt3hnM"
      },
      "source": [
        "mp_holistic = mp.solutions.holistic\n",
        "images_dir = '/home/arnav/Workarea/MentalStateProject/origin'\n",
        "images_dir_new = '/home/arnav/Workarea/MentalStateProject/origin2'\n",
        "i = 0\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "    for image_name in os.listdir(images_dir):\n",
        "        i += 1\n",
        "        if image_name.endswith(\".jpg\"):\n",
        "            expression_label = lookup(image_name)\n",
        "            image = cv2.imread(f'{images_dir}/{image_name}')\n",
        "            h, w, c = image.shape\n",
        "            if h > w:\n",
        "                image = cv2.copyMakeBorder(image, 0, 0, (h-w)//2, (h-w)//2 + 1, cv2.BORDER_CONSTANT, value=[255,255,255])\n",
        "            else:\n",
        "                image = cv2.copyMakeBorder(image, (w-h)//2, (w-h)//2 + 1, 0, 0, cv2.BORDER_CONSTANT, value=[255,255,255])\n",
        "            image = cv2.resize(image, (224, 224))\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            landmarks = holistic.process(image)\n",
        "            print(i)\n",
        "            print(image_name)\n",
        "            print(landmarks.face_landmarks)\n",
        "            if landmarks.face_landmarks is not None: # multiple faces returns None\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "                cv2.imwrite(f'{images_dir_new}/{expression_label}/{image_name}', image)\n",
        "                \n",
        "                facemesh = landmarks.face_landmarks.landmark # (x, y, z)\n",
        "\n",
        "                face_row = list(np.array([[landmark.x, landmark.y, landmark.z] for landmark in facemesh]).flatten())\n",
        "                face_row.insert(0, image_name)\n",
        "                face_row.insert(0, expression_label)\n",
        "\n",
        "                with open ('/home/arnav/Workarea/MentalStateProject/facemesh_landmarks2.csv', mode='a', newline='') as f:\n",
        "                    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "                    csv_writer.writerow(face_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwrDEdfmmPOG"
      },
      "source": [
        "# Preproccessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVD-hXCX4ARE"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/facemesh_landmarks.csv')\n",
        "filtered_df = df[df['expression_label'].notnull()]\n",
        "df = filtered_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQIRlPC_BoLu"
      },
      "source": [
        "for expression_labelï¼š\n",
        "\n",
        "\"0\" \"angry\"\n",
        "\n",
        "\"1\" \"disgust\"\n",
        "\n",
        "\"2\" \"fear\"\n",
        "\n",
        "\"3\" \"happy\"\n",
        "\n",
        "\"4\" \"sad\"\n",
        "\n",
        "\"5\" \"surprise\"\n",
        "\n",
        "\"6\" \"neutral\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YJzY32wEP2j"
      },
      "source": [
        "X = df.drop('expression_label', axis=1) # features\n",
        "y = df['expression_label'] # target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCs7a9uJYsVf"
      },
      "source": [
        "One hot encoding the class column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_MpptG31W1Y"
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "lb = LabelBinarizer()\n",
        "y = lb.fit_transform(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmb14smrsvMP"
      },
      "source": [
        "Data split pseudo-randomly (with seed) for 67% train, 33% test; stratified so same proportion of classes in train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zHdF5aCxQu_"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc1SmG8msE1Z"
      },
      "source": [
        "Using RandomUnderSampler, a sampling method to mitigate class imbalance (far more facemeshes for happy and sad compared to other classes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm7SKw7Vd1HN"
      },
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "rus = RandomUnderSampler(random_state=13)\n",
        "X_res, y_res = rus.fit_sample(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLoxG3LbyYJl"
      },
      "source": [
        "# ML Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaUincIqbK7g"
      },
      "source": [
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import InputLayer, Dense, Activation, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import Precision, Recall\n",
        "\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=X_res.shape[1]))\n",
        "model.add(Dense(y_res.shape[1]))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=1e-3) # keras default: lr=0.001\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=1000, validation_data = (X_test,y_test), batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_NUfsiyfGFC"
      },
      "source": [
        "model.save('simple_mlp.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90KIVnaJkt2L"
      },
      "source": [
        "from keras.models import load_model\n",
        "simple_mlp = load_model('/content/drive/MyDrive/simple_mlp.h5')\n",
        "simple_mlp.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoZxA8OycM3v"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=128)\n",
        "test_loss, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4568B4diGXkn"
      },
      "source": [
        "y_hat = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URT46c2t4QaI"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "y_hat=np.argmax(y_hat, axis=1)\n",
        "y_test=np.argmax(y_test, axis=1) \n",
        "cm = confusion_matrix(y_test, y_hat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9Bw6JMU5L-F"
      },
      "source": [
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3U8QG7gj0WP"
      },
      "source": [
        "import tensorflow as tf\n",
        "#from tf.losses import softmax_cross_entropy\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.optimizers import Adam\n",
        "import keras_tuner as kt\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Dense(2048, input_dim=X_train.shape[1]))\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(Dropout(0.1))\n",
        "model2.add(Dense(1024))\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(Dropout(0.1))\n",
        "model2.add(Dense(y_train.shape[1]))\n",
        "model2.add(Activation('softmax'))\n",
        "\n",
        "epochs = 10000\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-3,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "optimizer = keras.optimizers.Adam() # keras default: lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0\n",
        "#opt = tf.train.AdamOptimizer()\n",
        "model2.compile(\n",
        "    loss='categorical_crossentropy', \n",
        "    optimizer=optimizer, \n",
        "    metrics=['acc'])\n",
        "\n",
        "print(\"Training...\")\n",
        "model2.fit(X_train, y_train, steps_per_epoch=100, epochs=epochs, validation_data = (X_test,y_test), batch_size=64)\n",
        "\n",
        "model2.save('/content/drive/MyDrive/mlp.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tja3pPvenAe"
      },
      "source": [
        "y_hat = model2.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOmIgkx4JFY0"
      },
      "source": [
        "from keras.models import load_model\n",
        "model2 = load_model('/content/drive/MyDrive/mlp.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va6nHXKTBBoP"
      },
      "source": [
        "test_loss, test_acc = model2.evaluate(X_test, y_test, batch_size=128)\n",
        "test_loss, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RmH0WiWp056"
      },
      "source": [
        "#from tf.losses import softmax_cross_entropy\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import InputLayer, Dense, Activation, Dropout\n",
        "from keras.optimizers import Adam\n",
        "import kerastuner as kt\n",
        "\n",
        "def model_builder(hp):\n",
        "  model = Sequential()\n",
        "  model.add(InputLayer(input_shape=X_res.shape[1]))\n",
        "\n",
        "  model.add(Activation('relu'))\n",
        "  #model.add(Dropout(0.15))\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation('relu'))\n",
        "  #model.add(Dropout(0.15))\n",
        "  model.add(Dense(y_res.shape[1]))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "      initial_learning_rate=1e-2,\n",
        "      decay_steps=10000,\n",
        "      decay_rate=0.9)\n",
        "  optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "print(\"Training...\")\n",
        "model2.fit(X_res, y_res, epochs=100, validation_data = (X_test,y_test), batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6bNi4UkFJZ6"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, matthews_corrcoef, precision_score, recall_score, f1_score\n",
        "def compute_metrics(y_test, y_pred):\n",
        "    print('Accuracy: {:.5f}'.format(accuracy_score(y_test, y_pred)))\n",
        "    print('F-score: {:.5f}'.format(f1_score(y_test, y_pred)))\n",
        "    print('Precision: {:.5f}'.format(precision_score(y_test, y_pred)))\n",
        "    print('Recall: {:.5f}'.format(recall_score(y_test, y_pred)))\n",
        "    print('Accuracy (balanced): {:.5f}'.format(balanced_accuracy_score(y_test, y_pred)))\n",
        "    print('MCC: {:.5f}'.format(matthews_corrcoef(y_test, y_pred)))\n",
        "\n",
        "def compute_confusion_matrix(y_test, y_pred):\n",
        "    return pd.DataFrame(\n",
        "        confusion_matrix(y_test, y_pred, labels=[6, 5, 4, 3, 2, 1, 0]),\n",
        "        columns=['a(x) = 6','a(x) = 5','a(x) = 4','a(x) = 3','a(x) = 2','a(x) = 1', 'a(x) = 0'],\n",
        "        index=['y = 6', 'y = 5', 'y = 4', 'y = 3', 'y = 2', 'y = 1', 'y = 0'],\n",
        "    ).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2lIq-OqGK0Q"
      },
      "source": [
        "compute_confusion_matrix(y_test, y_hat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKoEn_7_XBK2"
      },
      "source": [
        "# Resnet to extract features concatenated to the facemesh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBhBNiEwcnDe"
      },
      "source": [
        "!unzip '/content/drive/MyDrive/FaceMesh/origin3.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8EzK6h-XFj-"
      },
      "source": [
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.resnet50 import preprocess_input\n",
        "from keras.applications.resnet50 import decode_predictions\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.models import Model\n",
        "from pathlib import *\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1oMqH6yAJwh"
      },
      "source": [
        "resnet = ResNet50(weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9xZ_GFEZ3un"
      },
      "source": [
        "# load model\n",
        "model = ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3), pooling='max')\n",
        "# remove the output layer\n",
        "#model = Model(inputs=model.inputs, outputs=model.layers[-2].output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grbVccowazwA"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKACcs5mdUP2"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/FaceMesh/facemesh_landmarks3.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX452n3keEdX"
      },
      "source": [
        "columns_to_add = []\n",
        "for i in range(features.shape[1]):\n",
        "  columns_to_add.append(f'feature{i}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1_TBUrjehuB"
      },
      "source": [
        "df[columns_to_add] = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOjLPYQRWd3q"
      },
      "source": [
        "num_coords = 468\n",
        "num_features = 2048\n",
        "header_row = ['expression_label', 'image_name']\n",
        "\n",
        "for i in range(1, num_coords+1):\n",
        "  header_row += [f'x{i}', f'y{i}', f'z{i}']\n",
        "\n",
        "for i in range(1, num_features+1):\n",
        "  header_row += [f'feature{i}']\n",
        "\n",
        "with open('/content/drive/MyDrive/FaceMesh shared with Mohammad/facemesh_and_features.csv', mode='w', newline='') as f:\n",
        "    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    csv_writer.writerow(header_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrUi36YnfUWG"
      },
      "source": [
        "for index, row in df.iterrows():\n",
        "  image_name = row['image_name']\n",
        "  expression_label = row['expression_label']\n",
        "  image = load_img(f'/content/origin3/{expression_label}/{image_name}', target_size=(224, 224))\n",
        "  # convert the image pixels to a numpy array\n",
        "  image = img_to_array(image)\n",
        "  # reshape data for the model\n",
        "  image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "  # prepare the image for the VGG model\n",
        "  image = preprocess_input(image)\n",
        "  # get extracted features\n",
        "  features = model.predict(image)\n",
        "  #feature = face_mesh + embedding\n",
        "  #output_csv.write(f'{feature}\\n')\n",
        "\n",
        "  facemesh = row.T\n",
        "\n",
        "  #features = np.reshape(features, (2048,))\n",
        "  #print(features.shape)\n",
        "  row_to_add = np.concatenate((row, features), axis=None)\n",
        "  print(index)\n",
        "  print(row_to_add)\n",
        "  \n",
        "  with open('/content/drive/MyDrive/FaceMesh shared with Mohammad/facemesh_and_features.csv', mode='a', newline='') as f:\n",
        "    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    csv_writer.writerow(row_to_add)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlHMfbXYeru4"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/FaceMesh shared with Mohammad/facemesh_and_features.csv')\n",
        "#filtered_df = df[df['expression_label'].notnull()]\n",
        "#df = filtered_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu4NQUh5OPil"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import minmax_scaler\n",
        "scaler = (MinMaxScaler((-1,1)))\n",
        "\n",
        "scaled = scaler.fit_transform()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5LI_HYseOQM"
      },
      "source": [
        "X = df.drop(['expression_label', 'image_name'], axis=1) # features\n",
        "y = df['expression_label'] # target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d82qeZVxecgD"
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "lb = LabelBinarizer()\n",
        "y = lb.fit_transform(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2BcK8BCeg6q"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa4pHDzVceY8"
      },
      "source": [
        "import tensorflow as tf\n",
        "#from tf.losses import softmax_cross_entropy\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import keras_tuner as kt\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(2048, input_dim=X_train.shape[1]))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(1024))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(y_train.shape[1]))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "epochs = 1000\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-3,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "optimizer = keras.optimizers.Adam() # keras default: lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0\n",
        "#opt = tf.train.AdamOptimizer()\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy', \n",
        "    optimizer=optimizer, \n",
        "    metrics=['acc'])\n",
        "\n",
        "print(\"Training...\")\n",
        "model.fit(X_train, y_train, steps_per_epoch=100, epochs=epochs, validation_data = (X_test,y_test), batch_size=64)\n",
        "\n",
        "#model.save('/content/drive/MyDrive/mlp.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV-JQYI4j9CI"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=128)\n",
        "test_loss, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gvy3jwtpnuEJ"
      },
      "source": [
        "# Making detections with model"
      ]
    }
  ]
}