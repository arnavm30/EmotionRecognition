{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Public_Facemesh_GNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Eg7aKj4j5P5V",
        "KfgDZBdy7lJe"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnavm30/EmotionRecognition/blob/main/Public_Facemesh_GNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9picfuWrF3_r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b47a1412-ac39-4552-e1d1-1490cc33a69f"
      },
      "source": [
        "# Install required packages.\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu111.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu111.html\n",
        "!pip install -q git+https://github.com/rusty1s/pytorch_geometric.git\n",
        "\n",
        "# Helper function for visualization.\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def visualize(h, color, epoch=None, loss=None):\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    if torch.is_tensor(h):\n",
        "        h = h.detach().cpu().numpy()\n",
        "        plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
        "        if epoch is not None and loss is not None:\n",
        "            plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n",
        "    else:\n",
        "        nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,\n",
        "                         node_color=color, cmap=\"Set2\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 10.4 MB 11.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 14.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 407 kB 12.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 4.1 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Y88cqnbs0FzS",
        "outputId": "7c6f48fe-1c33-43c4-a6b1-9c4cc5c149b9"
      },
      "source": [
        "torch.version.cuda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'11.1'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TsM5qS2xwgW",
        "outputId": "beb4e866-2fbe-462f-8845-8347cd36aaf1"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Oct 13 11:34:18 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgBIHHiHJlK_"
      },
      "source": [
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9q0gGDipScR"
      },
      "source": [
        "FACEMESH_LIPS = frozenset([(61, 146), (146, 91), (91, 181), (181, 84), (84, 17),\n",
        "                           (17, 314), (314, 405), (405, 321), (321, 375),\n",
        "                           (375, 291), (61, 185), (185, 40), (40, 39), (39, 37),\n",
        "                           (37, 0), (0, 267),\n",
        "                           (267, 269), (269, 270), (270, 409), (409, 291),\n",
        "                           (78, 95), (95, 88), (88, 178), (178, 87), (87, 14),\n",
        "                           (14, 317), (317, 402), (402, 318), (318, 324),\n",
        "                           (324, 308), (78, 191), (191, 80), (80, 81), (81, 82),\n",
        "                           (82, 13), (13, 312), (312, 311), (311, 310),\n",
        "                           (310, 415), (415, 308)])\n",
        "\n",
        "FACEMESH_LEFT_EYE = frozenset([(263, 249), (249, 390), (390, 373), (373, 374),\n",
        "                               (374, 380), (380, 381), (381, 382), (382, 362),\n",
        "                               (263, 466), (466, 388), (388, 387), (387, 386),\n",
        "                               (386, 385), (385, 384), (384, 398), (398, 362)])\n",
        "\n",
        "FACEMESH_LEFT_EYEBROW = frozenset([(276, 283), (283, 282), (282, 295),\n",
        "                                   (295, 285), (300, 293), (293, 334),\n",
        "                                   (334, 296), (296, 336)])\n",
        "\n",
        "FACEMESH_RIGHT_EYE = frozenset([(33, 7), (7, 163), (163, 144), (144, 145),\n",
        "                                (145, 153), (153, 154), (154, 155), (155, 133),\n",
        "                                (33, 246), (246, 161), (161, 160), (160, 159),\n",
        "                                (159, 158), (158, 157), (157, 173), (173, 133)])\n",
        "\n",
        "FACEMESH_RIGHT_EYEBROW = frozenset([(46, 53), (53, 52), (52, 65), (65, 55),\n",
        "                                    (70, 63), (63, 105), (105, 66), (66, 107)])\n",
        "\n",
        "FACEMESH_FACE_OVAL = frozenset([(10, 338), (338, 297), (297, 332), (332, 284),\n",
        "                                (284, 251), (251, 389), (389, 356), (356, 454),\n",
        "                                (454, 323), (323, 361), (361, 288), (288, 397),\n",
        "                                (397, 365), (365, 379), (379, 378), (378, 400),\n",
        "                                (400, 377), (377, 152), (152, 148), (148, 176),\n",
        "                                (176, 149), (149, 150), (150, 136), (136, 172),\n",
        "                                (172, 58), (58, 132), (132, 93), (93, 234),\n",
        "                                (234, 127), (127, 162), (162, 21), (21, 54),\n",
        "                                (54, 103), (103, 67), (67, 109), (109, 10)])\n",
        "\n",
        "FACEMESH_CONTOURS = frozenset().union(*[\n",
        "    FACEMESH_LIPS, FACEMESH_LEFT_EYE, FACEMESH_LEFT_EYEBROW, FACEMESH_RIGHT_EYE,\n",
        "    FACEMESH_RIGHT_EYEBROW, FACEMESH_FACE_OVAL\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvw6K2kvrCgt"
      },
      "source": [
        "# FACEMESH_CONTOURS = list(FACEMESH_CONTOURS)\n",
        "# FACEMESH_CONTOURS = [[a,b] for a,b in FACEMESH_CONTOURS]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdIuLU-irJtq"
      },
      "source": [
        "FACEMESH_TESSELATION = frozenset([\n",
        "    (127, 34),  (34, 139),  (139, 127), (11, 0),    (0, 37),    (37, 11),\n",
        "    (232, 231), (231, 120), (120, 232), (72, 37),   (37, 39),   (39, 72),\n",
        "    (128, 121), (121, 47),  (47, 128),  (232, 121), (121, 128), (128, 232),\n",
        "    (104, 69),  (69, 67),   (67, 104),  (175, 171), (171, 148), (148, 175),\n",
        "    (118, 50),  (50, 101),  (101, 118), (73, 39),   (39, 40),   (40, 73),\n",
        "    (9, 151),   (151, 108), (108, 9),   (48, 115),  (115, 131), (131, 48),\n",
        "    (194, 204), (204, 211), (211, 194), (74, 40),   (40, 185),  (185, 74),\n",
        "    (80, 42),   (42, 183),  (183, 80),  (40, 92),   (92, 186),  (186, 40),\n",
        "    (230, 229), (229, 118), (118, 230), (202, 212), (212, 214), (214, 202),\n",
        "    (83, 18),   (18, 17),   (17, 83),   (76, 61),   (61, 146),  (146, 76),\n",
        "    (160, 29),  (29, 30),   (30, 160),  (56, 157),  (157, 173), (173, 56),\n",
        "    (106, 204), (204, 194), (194, 106), (135, 214), (214, 192), (192, 135),\n",
        "    (203, 165), (165, 98),  (98, 203),  (21, 71),   (71, 68),   (68, 21),\n",
        "    (51, 45),   (45, 4),    (4, 51),    (144, 24),  (24, 23),   (23, 144),\n",
        "    (77, 146),  (146, 91),  (91, 77),   (205, 50),  (50, 187),  (187, 205),\n",
        "    (201, 200), (200, 18),  (18, 201),  (91, 106),  (106, 182), (182, 91),\n",
        "    (90, 91),   (91, 181),  (181, 90),  (85, 84),   (84, 17),   (17, 85),\n",
        "    (206, 203), (203, 36),  (36, 206),  (148, 171), (171, 140), (140, 148),\n",
        "    (92, 40),   (40, 39),   (39, 92),   (193, 189), (189, 244), (244, 193),\n",
        "    (159, 158), (158, 28),  (28, 159),  (247, 246), (246, 161), (161, 247),\n",
        "    (236, 3),   (3, 196),   (196, 236), (54, 68),   (68, 104),  (104, 54),\n",
        "    (193, 168), (168, 8),   (8, 193),   (117, 228), (228, 31),  (31, 117),\n",
        "    (189, 193), (193, 55),  (55, 189),  (98, 97),   (97, 99),   (99, 98),\n",
        "    (126, 47),  (47, 100),  (100, 126), (166, 79),  (79, 218),  (218, 166),\n",
        "    (155, 154), (154, 26),  (26, 155),  (209, 49),  (49, 131),  (131, 209),\n",
        "    (135, 136), (136, 150), (150, 135), (47, 126),  (126, 217), (217, 47),\n",
        "    (223, 52),  (52, 53),   (53, 223),  (45, 51),   (51, 134),  (134, 45),\n",
        "    (211, 170), (170, 140), (140, 211), (67, 69),   (69, 108),  (108, 67),\n",
        "    (43, 106),  (106, 91),  (91, 43),   (230, 119), (119, 120), (120, 230),\n",
        "    (226, 130), (130, 247), (247, 226), (63, 53),   (53, 52),   (52, 63),\n",
        "    (238, 20),  (20, 242),  (242, 238), (46, 70),   (70, 156),  (156, 46),\n",
        "    (78, 62),   (62, 96),   (96, 78),   (46, 53),   (53, 63),   (63, 46),\n",
        "    (143, 34),  (34, 227),  (227, 143), (123, 117), (117, 111), (111, 123),\n",
        "    (44, 125),  (125, 19),  (19, 44),   (236, 134), (134, 51),  (51, 236),\n",
        "    (216, 206), (206, 205), (205, 216), (154, 153), (153, 22),  (22, 154),\n",
        "    (39, 37),   (37, 167),  (167, 39),  (200, 201), (201, 208), (208, 200),\n",
        "    (36, 142),  (142, 100), (100, 36),  (57, 212),  (212, 202), (202, 57),\n",
        "    (20, 60),   (60, 99),   (99, 20),   (28, 158),  (158, 157), (157, 28),\n",
        "    (35, 226),  (226, 113), (113, 35),  (160, 159), (159, 27),  (27, 160),\n",
        "    (204, 202), (202, 210), (210, 204), (113, 225), (225, 46),  (46, 113),\n",
        "    (43, 202),  (202, 204), (204, 43),  (62, 76),   (76, 77),   (77, 62),\n",
        "    (137, 123), (123, 116), (116, 137), (41, 38),   (38, 72),   (72, 41),\n",
        "    (203, 129), (129, 142), (142, 203), (64, 98),   (98, 240),  (240, 64),\n",
        "    (49, 102),  (102, 64),  (64, 49),   (41, 73),   (73, 74),   (74, 41),\n",
        "    (212, 216), (216, 207), (207, 212), (42, 74),   (74, 184),  (184, 42),\n",
        "    (169, 170), (170, 211), (211, 169), (170, 149), (149, 176), (176, 170),\n",
        "    (105, 66),  (66, 69),   (69, 105),  (122, 6),   (6, 168),   (168, 122),\n",
        "    (123, 147), (147, 187), (187, 123), (96, 77),   (77, 90),   (90, 96),\n",
        "    (65, 55),   (55, 107),  (107, 65),  (89, 90),   (90, 180),  (180, 89),\n",
        "    (101, 100), (100, 120), (120, 101), (63, 105),  (105, 104), (104, 63),\n",
        "    (93, 137),  (137, 227), (227, 93),  (15, 86),   (86, 85),   (85, 15),\n",
        "    (129, 102), (102, 49),  (49, 129),  (14, 87),   (87, 86),   (86, 14),\n",
        "    (55, 8),    (8, 9),     (9, 55),    (100, 47),  (47, 121),  (121, 100),\n",
        "    (145, 23),  (23, 22),   (22, 145),  (88, 89),   (89, 179),  (179, 88),\n",
        "    (6, 122),   (122, 196), (196, 6),   (88, 95),   (95, 96),   (96, 88),\n",
        "    (138, 172), (172, 136), (136, 138), (215, 58),  (58, 172),  (172, 215),\n",
        "    (115, 48),  (48, 219),  (219, 115), (42, 80),   (80, 81),   (81, 42),\n",
        "    (195, 3),   (3, 51),    (51, 195),  (43, 146),  (146, 61),  (61, 43),\n",
        "    (171, 175), (175, 199), (199, 171), (81, 82),   (82, 38),   (38, 81),\n",
        "    (53, 46),   (46, 225),  (225, 53),  (144, 163), (163, 110), (110, 144),\n",
        "    (52, 65),   (65, 66),   (66, 52),   (229, 228), (228, 117), (117, 229),\n",
        "    (34, 127),  (127, 234), (234, 34),  (107, 108), (108, 69),  (69, 107),\n",
        "    (109, 108), (108, 151), (151, 109), (48, 64),   (64, 235),  (235, 48),\n",
        "    (62, 78),   (78, 191),  (191, 62),  (129, 209), (209, 126), (126, 129),\n",
        "    (111, 35),  (35, 143),  (143, 111), (117, 123), (123, 50),  (50, 117),\n",
        "    (222, 65),  (65, 52),   (52, 222),  (19, 125),  (125, 141), (141, 19),\n",
        "    (221, 55),  (55, 65),   (65, 221),  (3, 195),   (195, 197), (197, 3),\n",
        "    (25, 7),    (7, 33),    (33, 25),   (220, 237), (237, 44),  (44, 220),\n",
        "    (70, 71),   (71, 139),  (139, 70),  (122, 193), (193, 245), (245, 122),\n",
        "    (247, 130), (130, 33),  (33, 247),  (71, 21),   (21, 162),  (162, 71),\n",
        "    (170, 169), (169, 150), (150, 170), (188, 174), (174, 196), (196, 188),\n",
        "    (216, 186), (186, 92),  (92, 216),  (2, 97),    (97, 167),  (167, 2),\n",
        "    (141, 125), (125, 241), (241, 141), (164, 167), (167, 37),  (37, 164),\n",
        "    (72, 38),   (38, 12),   (12, 72),   (38, 82),   (82, 13),   (13, 38),\n",
        "    (63, 68),   (68, 71),   (71, 63),   (226, 35),  (35, 111),  (111, 226),\n",
        "    (101, 50),  (50, 205),  (205, 101), (206, 92),  (92, 165),  (165, 206),\n",
        "    (209, 198), (198, 217), (217, 209), (165, 167), (167, 97),  (97, 165),\n",
        "    (220, 115), (115, 218), (218, 220), (133, 112), (112, 243), (243, 133),\n",
        "    (239, 238), (238, 241), (241, 239), (214, 135), (135, 169), (169, 214),\n",
        "    (190, 173), (173, 133), (133, 190), (171, 208), (208, 32),  (32, 171),\n",
        "    (125, 44),  (44, 237),  (237, 125), (86, 87),   (87, 178),  (178, 86),\n",
        "    (85, 86),   (86, 179),  (179, 85),  (84, 85),   (85, 180),  (180, 84),\n",
        "    (83, 84),   (84, 181),  (181, 83),  (201, 83),  (83, 182),  (182, 201),\n",
        "    (137, 93),  (93, 132),  (132, 137), (76, 62),   (62, 183),  (183, 76),\n",
        "    (61, 76),   (76, 184),  (184, 61),  (57, 61),   (61, 185),  (185, 57),\n",
        "    (212, 57),  (57, 186),  (186, 212), (214, 207), (207, 187), (187, 214),\n",
        "    (34, 143),  (143, 156), (156, 34),  (79, 239),  (239, 237), (237, 79),\n",
        "    (123, 137), (137, 177), (177, 123), (44, 1),    (1, 4),     (4, 44),\n",
        "    (201, 194), (194, 32),  (32, 201),  (64, 102),  (102, 129), (129, 64),\n",
        "    (213, 215), (215, 138), (138, 213), (59, 166),  (166, 219), (219, 59),\n",
        "    (242, 99),  (99, 97),   (97, 242),  (2, 94),    (94, 141),  (141, 2),\n",
        "    (75, 59),   (59, 235),  (235, 75),  (24, 110),  (110, 228), (228, 24),\n",
        "    (25, 130),  (130, 226), (226, 25),  (23, 24),   (24, 229),  (229, 23),\n",
        "    (22, 23),   (23, 230),  (230, 22),  (26, 22),   (22, 231),  (231, 26),\n",
        "    (112, 26),  (26, 232),  (232, 112), (189, 190), (190, 243), (243, 189),\n",
        "    (221, 56),  (56, 190),  (190, 221), (28, 56),   (56, 221),  (221, 28),\n",
        "    (27, 28),   (28, 222),  (222, 27),  (29, 27),   (27, 223),  (223, 29),\n",
        "    (30, 29),   (29, 224),  (224, 30),  (247, 30),  (30, 225),  (225, 247),\n",
        "    (238, 79),  (79, 20),   (20, 238),  (166, 59),  (59, 75),   (75, 166),\n",
        "    (60, 75),   (75, 240),  (240, 60),  (147, 177), (177, 215), (215, 147),\n",
        "    (20, 79),   (79, 166),  (166, 20),  (187, 147), (147, 213), (213, 187),\n",
        "    (112, 233), (233, 244), (244, 112), (233, 128), (128, 245), (245, 233),\n",
        "    (128, 114), (114, 188), (188, 128), (114, 217), (217, 174), (174, 114),\n",
        "    (131, 115), (115, 220), (220, 131), (217, 198), (198, 236), (236, 217),\n",
        "    (198, 131), (131, 134), (134, 198), (177, 132), (132, 58),  (58, 177),\n",
        "    (143, 35),  (35, 124),  (124, 143), (110, 163), (163, 7),   (7, 110),\n",
        "    (228, 110), (110, 25),  (25, 228),  (356, 389), (389, 368), (368, 356),\n",
        "    (11, 302),  (302, 267), (267, 11),  (452, 350), (350, 349), (349, 452),\n",
        "    (302, 303), (303, 269), (269, 302), (357, 343), (343, 277), (277, 357),\n",
        "    (452, 453), (453, 357), (357, 452), (333, 332), (332, 297), (297, 333),\n",
        "    (175, 152), (152, 377), (377, 175), (347, 348), (348, 330), (330, 347),\n",
        "    (303, 304), (304, 270), (270, 303), (9, 336),   (336, 337), (337, 9),\n",
        "    (278, 279), (279, 360), (360, 278), (418, 262), (262, 431), (431, 418),\n",
        "    (304, 408), (408, 409), (409, 304), (310, 415), (415, 407), (407, 310),\n",
        "    (270, 409), (409, 410), (410, 270), (450, 348), (348, 347), (347, 450),\n",
        "    (422, 430), (430, 434), (434, 422), (313, 314), (314, 17),  (17, 313),\n",
        "    (306, 307), (307, 375), (375, 306), (387, 388), (388, 260), (260, 387),\n",
        "    (286, 414), (414, 398), (398, 286), (335, 406), (406, 418), (418, 335),\n",
        "    (364, 367), (367, 416), (416, 364), (423, 358), (358, 327), (327, 423),\n",
        "    (251, 284), (284, 298), (298, 251), (281, 5),   (5, 4),     (4, 281),\n",
        "    (373, 374), (374, 253), (253, 373), (307, 320), (320, 321), (321, 307),\n",
        "    (425, 427), (427, 411), (411, 425), (421, 313), (313, 18),  (18, 421),\n",
        "    (321, 405), (405, 406), (406, 321), (320, 404), (404, 405), (405, 320),\n",
        "    (315, 16),  (16, 17),   (17, 315),  (426, 425), (425, 266), (266, 426),\n",
        "    (377, 400), (400, 369), (369, 377), (322, 391), (391, 269), (269, 322),\n",
        "    (417, 465), (465, 464), (464, 417), (386, 257), (257, 258), (258, 386),\n",
        "    (466, 260), (260, 388), (388, 466), (456, 399), (399, 419), (419, 456),\n",
        "    (284, 332), (332, 333), (333, 284), (417, 285), (285, 8),   (8, 417),\n",
        "    (346, 340), (340, 261), (261, 346), (413, 441), (441, 285), (285, 413),\n",
        "    (327, 460), (460, 328), (328, 327), (355, 371), (371, 329), (329, 355),\n",
        "    (392, 439), (439, 438), (438, 392), (382, 341), (341, 256), (256, 382),\n",
        "    (429, 420), (420, 360), (360, 429), (364, 394), (394, 379), (379, 364),\n",
        "    (277, 343), (343, 437), (437, 277), (443, 444), (444, 283), (283, 443),\n",
        "    (275, 440), (440, 363), (363, 275), (431, 262), (262, 369), (369, 431),\n",
        "    (297, 338), (338, 337), (337, 297), (273, 375), (375, 321), (321, 273),\n",
        "    (450, 451), (451, 349), (349, 450), (446, 342), (342, 467), (467, 446),\n",
        "    (293, 334), (334, 282), (282, 293), (458, 461), (461, 462), (462, 458),\n",
        "    (276, 353), (353, 383), (383, 276), (308, 324), (324, 325), (325, 308),\n",
        "    (276, 300), (300, 293), (293, 276), (372, 345), (345, 447), (447, 372),\n",
        "    (352, 345), (345, 340), (340, 352), (274, 1),   (1, 19),    (19, 274),\n",
        "    (456, 248), (248, 281), (281, 456), (436, 427), (427, 425), (425, 436),\n",
        "    (381, 256), (256, 252), (252, 381), (269, 391), (391, 393), (393, 269),\n",
        "    (200, 199), (199, 428), (428, 200), (266, 330), (330, 329), (329, 266),\n",
        "    (287, 273), (273, 422), (422, 287), (250, 462), (462, 328), (328, 250),\n",
        "    (258, 286), (286, 384), (384, 258), (265, 353), (353, 342), (342, 265),\n",
        "    (387, 259), (259, 257), (257, 387), (424, 431), (431, 430), (430, 424),\n",
        "    (342, 353), (353, 276), (276, 342), (273, 335), (335, 424), (424, 273),\n",
        "    (292, 325), (325, 307), (307, 292), (366, 447), (447, 345), (345, 366),\n",
        "    (271, 303), (303, 302), (302, 271), (423, 266), (266, 371), (371, 423),\n",
        "    (294, 455), (455, 460), (460, 294), (279, 278), (278, 294), (294, 279),\n",
        "    (271, 272), (272, 304), (304, 271), (432, 434), (434, 427), (427, 432),\n",
        "    (272, 407), (407, 408), (408, 272), (394, 430), (430, 431), (431, 394),\n",
        "    (395, 369), (369, 400), (400, 395), (334, 333), (333, 299), (299, 334),\n",
        "    (351, 417), (417, 168), (168, 351), (352, 280), (280, 411), (411, 352),\n",
        "    (325, 319), (319, 320), (320, 325), (295, 296), (296, 336), (336, 295),\n",
        "    (319, 403), (403, 404), (404, 319), (330, 348), (348, 349), (349, 330),\n",
        "    (293, 298), (298, 333), (333, 293), (323, 454), (454, 447), (447, 323),\n",
        "    (15, 16),   (16, 315),  (315, 15),  (358, 429), (429, 279), (279, 358),\n",
        "    (14, 15),   (15, 316),  (316, 14),  (285, 336), (336, 9),   (9, 285),\n",
        "    (329, 349), (349, 350), (350, 329), (374, 380), (380, 252), (252, 374),\n",
        "    (318, 402), (402, 403), (403, 318), (6, 197),   (197, 419), (419, 6),\n",
        "    (318, 319), (319, 325), (325, 318), (367, 364), (364, 365), (365, 367),\n",
        "    (435, 367), (367, 397), (397, 435), (344, 438), (438, 439), (439, 344),\n",
        "    (272, 271), (271, 311), (311, 272), (195, 5),   (5, 281),   (281, 195),\n",
        "    (273, 287), (287, 291), (291, 273), (396, 428), (428, 199), (199, 396),\n",
        "    (311, 271), (271, 268), (268, 311), (283, 444), (444, 445), (445, 283),\n",
        "    (373, 254), (254, 339), (339, 373), (282, 334), (334, 296), (296, 282),\n",
        "    (449, 347), (347, 346), (346, 449), (264, 447), (447, 454), (454, 264),\n",
        "    (336, 296), (296, 299), (299, 336), (338, 10),  (10, 151),  (151, 338),\n",
        "    (278, 439), (439, 455), (455, 278), (292, 407), (407, 415), (415, 292),\n",
        "    (358, 371), (371, 355), (355, 358), (340, 345), (345, 372), (372, 340),\n",
        "    (346, 347), (347, 280), (280, 346), (442, 443), (443, 282), (282, 442),\n",
        "    (19, 94),   (94, 370),  (370, 19),  (441, 442), (442, 295), (295, 441),\n",
        "    (248, 419), (419, 197), (197, 248), (263, 255), (255, 359), (359, 263),\n",
        "    (440, 275), (275, 274), (274, 440), (300, 383), (383, 368), (368, 300),\n",
        "    (351, 412), (412, 465), (465, 351), (263, 467), (467, 466), (466, 263),\n",
        "    (301, 368), (368, 389), (389, 301), (395, 378), (378, 379), (379, 395),\n",
        "    (412, 351), (351, 419), (419, 412), (436, 426), (426, 322), (322, 436),\n",
        "    (2, 164),   (164, 393), (393, 2),   (370, 462), (462, 461), (461, 370),\n",
        "    (164, 0),   (0, 267),   (267, 164), (302, 11),  (11, 12),   (12, 302),\n",
        "    (268, 12),  (12, 13),   (13, 268),  (293, 300), (300, 301), (301, 293),\n",
        "    (446, 261), (261, 340), (340, 446), (330, 266), (266, 425), (425, 330),\n",
        "    (426, 423), (423, 391), (391, 426), (429, 355), (355, 437), (437, 429),\n",
        "    (391, 327), (327, 326), (326, 391), (440, 457), (457, 438), (438, 440),\n",
        "    (341, 382), (382, 362), (362, 341), (459, 457), (457, 461), (461, 459),\n",
        "    (434, 430), (430, 394), (394, 434), (414, 463), (463, 362), (362, 414),\n",
        "    (396, 369), (369, 262), (262, 396), (354, 461), (461, 457), (457, 354),\n",
        "    (316, 403), (403, 402), (402, 316), (315, 404), (404, 403), (403, 315),\n",
        "    (314, 405), (405, 404), (404, 314), (313, 406), (406, 405), (405, 313),\n",
        "    (421, 418), (418, 406), (406, 421), (366, 401), (401, 361), (361, 366),\n",
        "    (306, 408), (408, 407), (407, 306), (291, 409), (409, 408), (408, 291),\n",
        "    (287, 410), (410, 409), (409, 287), (432, 436), (436, 410), (410, 432),\n",
        "    (434, 416), (416, 411), (411, 434), (264, 368), (368, 383), (383, 264),\n",
        "    (309, 438), (438, 457), (457, 309), (352, 376), (376, 401), (401, 352),\n",
        "    (274, 275), (275, 4),   (4, 274),   (421, 428), (428, 262), (262, 421),\n",
        "    (294, 327), (327, 358), (358, 294), (433, 416), (416, 367), (367, 433),\n",
        "    (289, 455), (455, 439), (439, 289), (462, 370), (370, 326), (326, 462),\n",
        "    (2, 326),   (326, 370), (370, 2),   (305, 460), (460, 455), (455, 305),\n",
        "    (254, 449), (449, 448), (448, 254), (255, 261), (261, 446), (446, 255),\n",
        "    (253, 450), (450, 449), (449, 253), (252, 451), (451, 450), (450, 252),\n",
        "    (256, 452), (452, 451), (451, 256), (341, 453), (453, 452), (452, 341),\n",
        "    (413, 464), (464, 463), (463, 413), (441, 413), (413, 414), (414, 441),\n",
        "    (258, 442), (442, 441), (441, 258), (257, 443), (443, 442), (442, 257),\n",
        "    (259, 444), (444, 443), (443, 259), (260, 445), (445, 444), (444, 260),\n",
        "    (467, 342), (342, 445), (445, 467), (459, 458), (458, 250), (250, 459),\n",
        "    (289, 392), (392, 290), (290, 289), (290, 328), (328, 460), (460, 290),\n",
        "    (376, 433), (433, 435), (435, 376), (250, 290), (290, 392), (392, 250),\n",
        "    (411, 416), (416, 433), (433, 411), (341, 463), (463, 464), (464, 341),\n",
        "    (453, 464), (464, 465), (465, 453), (357, 465), (465, 412), (412, 357),\n",
        "    (343, 412), (412, 399), (399, 343), (360, 363), (363, 440), (440, 360),\n",
        "    (437, 399), (399, 456), (456, 437), (420, 456), (456, 363), (363, 420),\n",
        "    (401, 435), (435, 288), (288, 401), (372, 383), (383, 353), (353, 372),\n",
        "    (339, 255), (255, 249), (249, 339), (448, 261), (261, 255), (255, 448),\n",
        "    (133, 243), (243, 190), (190, 133), (133, 155), (155, 112), (112, 133),\n",
        "    (33, 246),  (246, 247), (247, 33),  (33, 130),  (130, 25),  (25, 33),\n",
        "    (398, 384), (384, 286), (286, 398), (362, 398), (398, 414), (414, 362),\n",
        "    (362, 463), (463, 341), (341, 362), (263, 359), (359, 467), (467, 263),\n",
        "    (263, 249), (249, 255), (255, 263), (466, 467), (467, 260), (260, 466),\n",
        "    (75, 60),   (60, 166),  (166, 75),  (238, 239), (239, 79),  (79, 238),\n",
        "    (162, 127), (127, 139), (139, 162), (72, 11),   (11, 37),   (37, 72),\n",
        "    (121, 232), (232, 120), (120, 121), (73, 72),   (72, 39),   (39, 73),\n",
        "    (114, 128), (128, 47),  (47, 114),  (233, 232), (232, 128), (128, 233),\n",
        "    (103, 104), (104, 67),  (67, 103),  (152, 175), (175, 148), (148, 152),\n",
        "    (119, 118), (118, 101), (101, 119), (74, 73),   (73, 40),   (40, 74),\n",
        "    (107, 9),   (9, 108),   (108, 107), (49, 48),   (48, 131),  (131, 49),\n",
        "    (32, 194),  (194, 211), (211, 32),  (184, 74),  (74, 185),  (185, 184),\n",
        "    (191, 80),  (80, 183),  (183, 191), (185, 40),  (40, 186),  (186, 185),\n",
        "    (119, 230), (230, 118), (118, 119), (210, 202), (202, 214), (214, 210),\n",
        "    (84, 83),   (83, 17),   (17, 84),   (77, 76),   (76, 146),  (146, 77),\n",
        "    (161, 160), (160, 30),  (30, 161),  (190, 56),  (56, 173),  (173, 190),\n",
        "    (182, 106), (106, 194), (194, 182), (138, 135), (135, 192), (192, 138),\n",
        "    (129, 203), (203, 98),  (98, 129),  (54, 21),   (21, 68),   (68, 54),\n",
        "    (5, 51),    (51, 4),    (4, 5),     (145, 144), (144, 23),  (23, 145),\n",
        "    (90, 77),   (77, 91),   (91, 90),   (207, 205), (205, 187), (187, 207),\n",
        "    (83, 201),  (201, 18),  (18, 83),   (181, 91),  (91, 182),  (182, 181),\n",
        "    (180, 90),  (90, 181),  (181, 180), (16, 85),   (85, 17),   (17, 16),\n",
        "    (205, 206), (206, 36),  (36, 205),  (176, 148), (148, 140), (140, 176),\n",
        "    (165, 92),  (92, 39),   (39, 165),  (245, 193), (193, 244), (244, 245),\n",
        "    (27, 159),  (159, 28),  (28, 27),   (30, 247),  (247, 161), (161, 30),\n",
        "    (174, 236), (236, 196), (196, 174), (103, 54),  (54, 104),  (104, 103),\n",
        "    (55, 193),  (193, 8),   (8, 55),    (111, 117), (117, 31),  (31, 111),\n",
        "    (221, 189), (189, 55),  (55, 221),  (240, 98),  (98, 99),   (99, 240),\n",
        "    (142, 126), (126, 100), (100, 142), (219, 166), (166, 218), (218, 219),\n",
        "    (112, 155), (155, 26),  (26, 112),  (198, 209), (209, 131), (131, 198),\n",
        "    (169, 135), (135, 150), (150, 169), (114, 47),  (47, 217),  (217, 114),\n",
        "    (224, 223), (223, 53),  (53, 224),  (220, 45),  (45, 134),  (134, 220),\n",
        "    (32, 211),  (211, 140), (140, 32),  (109, 67),  (67, 108),  (108, 109),\n",
        "    (146, 43),  (43, 91),   (91, 146),  (231, 230), (230, 120), (120, 231),\n",
        "    (113, 226), (226, 247), (247, 113), (105, 63),  (63, 52),   (52, 105),\n",
        "    (241, 238), (238, 242), (242, 241), (124, 46),  (46, 156),  (156, 124),\n",
        "    (95, 78),   (78, 96),   (96, 95),   (70, 46),   (46, 63),   (63, 70),\n",
        "    (116, 143), (143, 227), (227, 116), (116, 123), (123, 111), (111, 116),\n",
        "    (1, 44),    (44, 19),   (19, 1),    (3, 236),   (236, 51),  (51, 3),\n",
        "    (207, 216), (216, 205), (205, 207), (26, 154),  (154, 22),  (22, 26),\n",
        "    (165, 39),  (39, 167),  (167, 165), (199, 200), (200, 208), (208, 199),\n",
        "    (101, 36),  (36, 100),  (100, 101), (43, 57),   (57, 202),  (202, 43),\n",
        "    (242, 20),  (20, 99),   (99, 242),  (56, 28),   (28, 157),  (157, 56),\n",
        "    (124, 35),  (35, 113),  (113, 124), (29, 160),  (160, 27),  (27, 29),\n",
        "    (211, 204), (204, 210), (210, 211), (124, 113), (113, 46),  (46, 124),\n",
        "    (106, 43),  (43, 204),  (204, 106), (96, 62),   (62, 77),   (77, 96),\n",
        "    (227, 137), (137, 116), (116, 227), (73, 41),   (41, 72),   (72, 73),\n",
        "    (36, 203),  (203, 142), (142, 36),  (235, 64),  (64, 240),  (240, 235),\n",
        "    (48, 49),   (49, 64),   (64, 48),   (42, 41),   (41, 74),   (74, 42),\n",
        "    (214, 212), (212, 207), (207, 214), (183, 42),  (42, 184),  (184, 183),\n",
        "    (210, 169), (169, 211), (211, 210), (140, 170), (170, 176), (176, 140),\n",
        "    (104, 105), (105, 69),  (69, 104),  (193, 122), (122, 168), (168, 193),\n",
        "    (50, 123),  (123, 187), (187, 50),  (89, 96),   (96, 90),   (90, 89),\n",
        "    (66, 65),   (65, 107),  (107, 66),  (179, 89),  (89, 180),  (180, 179),\n",
        "    (119, 101), (101, 120), (120, 119), (68, 63),   (63, 104),  (104, 68),\n",
        "    (234, 93),  (93, 227),  (227, 234), (16, 15),   (15, 85),   (85, 16),\n",
        "    (209, 129), (129, 49),  (49, 209),  (15, 14),   (14, 86),   (86, 15),\n",
        "    (107, 55),  (55, 9),    (9, 107),   (120, 100), (100, 121), (121, 120),\n",
        "    (153, 145), (145, 22),  (22, 153),  (178, 88),  (88, 179),  (179, 178),\n",
        "    (197, 6),   (6, 196),   (196, 197), (89, 88),   (88, 96),   (96, 89),\n",
        "    (135, 138), (138, 136), (136, 135), (138, 215), (215, 172), (172, 138),\n",
        "    (218, 115), (115, 219), (219, 218), (41, 42),   (42, 81),   (81, 41),\n",
        "    (5, 195),   (195, 51),  (51, 5),    (57, 43),   (43, 61),   (61, 57),\n",
        "    (208, 171), (171, 199), (199, 208), (41, 81),   (81, 38),   (38, 41),\n",
        "    (224, 53),  (53, 225),  (225, 224), (24, 144),  (144, 110), (110, 24),\n",
        "    (105, 52),  (52, 66),   (66, 105),  (118, 229), (229, 117), (117, 118),\n",
        "    (227, 34),  (34, 234),  (234, 227), (66, 107),  (107, 69),  (69, 66),\n",
        "    (10, 109),  (109, 151), (151, 10),  (219, 48),  (48, 235),  (235, 219),\n",
        "    (183, 62),  (62, 191),  (191, 183), (142, 129), (129, 126), (126, 142),\n",
        "    (116, 111), (111, 143), (143, 116), (118, 117), (117, 50),  (50, 118),\n",
        "    (223, 222), (222, 52),  (52, 223),  (94, 19),   (19, 141),  (141, 94),\n",
        "    (222, 221), (221, 65),  (65, 222),  (196, 3),   (3, 197),   (197, 196),\n",
        "    (45, 220),  (220, 44),  (44, 45),   (156, 70),  (70, 139),  (139, 156),\n",
        "    (188, 122), (122, 245), (245, 188), (139, 71),  (71, 162),  (162, 139),\n",
        "    (149, 170), (170, 150), (150, 149), (122, 188), (188, 196), (196, 122),\n",
        "    (206, 216), (216, 92),  (92, 206),  (164, 2),   (2, 167),   (167, 164),\n",
        "    (242, 141), (141, 241), (241, 242), (0, 164),   (164, 37),  (37, 0),\n",
        "    (11, 72),   (72, 12),   (12, 11),   (12, 38),   (38, 13),   (13, 12),\n",
        "    (70, 63),   (63, 71),   (71, 70),   (31, 226),  (226, 111), (111, 31),\n",
        "    (36, 101),  (101, 205), (205, 36),  (203, 206), (206, 165), (165, 203),\n",
        "    (126, 209), (209, 217), (217, 126), (98, 165),  (165, 97),  (97, 98),\n",
        "    (237, 220), (220, 218), (218, 237), (237, 239), (239, 241), (241, 237),\n",
        "    (210, 214), (214, 169), (169, 210), (140, 171), (171, 32),  (32, 140),\n",
        "    (241, 125), (125, 237), (237, 241), (179, 86),  (86, 178),  (178, 179),\n",
        "    (180, 85),  (85, 179),  (179, 180), (181, 84),  (84, 180),  (180, 181),\n",
        "    (182, 83),  (83, 181),  (181, 182), (194, 201), (201, 182), (182, 194),\n",
        "    (177, 137), (137, 132), (132, 177), (184, 76),  (76, 183),  (183, 184),\n",
        "    (185, 61),  (61, 184),  (184, 185), (186, 57),  (57, 185),  (185, 186),\n",
        "    (216, 212), (212, 186), (186, 216), (192, 214), (214, 187), (187, 192),\n",
        "    (139, 34),  (34, 156),  (156, 139), (218, 79),  (79, 237),  (237, 218),\n",
        "    (147, 123), (123, 177), (177, 147), (45, 44),   (44, 4),    (4, 45),\n",
        "    (208, 201), (201, 32),  (32, 208),  (98, 64),   (64, 129),  (129, 98),\n",
        "    (192, 213), (213, 138), (138, 192), (235, 59),  (59, 219),  (219, 235),\n",
        "    (141, 242), (242, 97),  (97, 141),  (97, 2),    (2, 141),   (141, 97),\n",
        "    (240, 75),  (75, 235),  (235, 240), (229, 24),  (24, 228),  (228, 229),\n",
        "    (31, 25),   (25, 226),  (226, 31),  (230, 23),  (23, 229),  (229, 230),\n",
        "    (231, 22),  (22, 230),  (230, 231), (232, 26),  (26, 231),  (231, 232),\n",
        "    (233, 112), (112, 232), (232, 233), (244, 189), (189, 243), (243, 244),\n",
        "    (189, 221), (221, 190), (190, 189), (222, 28),  (28, 221),  (221, 222),\n",
        "    (223, 27),  (27, 222),  (222, 223), (224, 29),  (29, 223),  (223, 224),\n",
        "    (225, 30),  (30, 224),  (224, 225), (113, 247), (247, 225), (225, 113),\n",
        "    (99, 60),   (60, 240),  (240, 99),  (213, 147), (147, 215), (215, 213),\n",
        "    (60, 20),   (20, 166),  (166, 60),  (192, 187), (187, 213), (213, 192),\n",
        "    (243, 112), (112, 244), (244, 243), (244, 233), (233, 245), (245, 244),\n",
        "    (245, 128), (128, 188), (188, 245), (188, 114), (114, 174), (174, 188),\n",
        "    (134, 131), (131, 220), (220, 134), (174, 217), (217, 236), (236, 174),\n",
        "    (236, 198), (198, 134), (134, 236), (215, 177), (177, 58),  (58, 215),\n",
        "    (156, 143), (143, 124), (124, 156), (25, 110),  (110, 7),   (7, 25),\n",
        "    (31, 228),  (228, 25),  (25, 31),   (264, 356), (356, 368), (368, 264),\n",
        "    (0, 11),    (11, 267),  (267, 0),   (451, 452), (452, 349), (349, 451),\n",
        "    (267, 302), (302, 269), (269, 267), (350, 357), (357, 277), (277, 350),\n",
        "    (350, 452), (452, 357), (357, 350), (299, 333), (333, 297), (297, 299),\n",
        "    (396, 175), (175, 377), (377, 396), (280, 347), (347, 330), (330, 280),\n",
        "    (269, 303), (303, 270), (270, 269), (151, 9),   (9, 337),   (337, 151),\n",
        "    (344, 278), (278, 360), (360, 344), (424, 418), (418, 431), (431, 424),\n",
        "    (270, 304), (304, 409), (409, 270), (272, 310), (310, 407), (407, 272),\n",
        "    (322, 270), (270, 410), (410, 322), (449, 450), (450, 347), (347, 449),\n",
        "    (432, 422), (422, 434), (434, 432), (18, 313),  (313, 17),  (17, 18),\n",
        "    (291, 306), (306, 375), (375, 291), (259, 387), (387, 260), (260, 259),\n",
        "    (424, 335), (335, 418), (418, 424), (434, 364), (364, 416), (416, 434),\n",
        "    (391, 423), (423, 327), (327, 391), (301, 251), (251, 298), (298, 301),\n",
        "    (275, 281), (281, 4),   (4, 275),   (254, 373), (373, 253), (253, 254),\n",
        "    (375, 307), (307, 321), (321, 375), (280, 425), (425, 411), (411, 280),\n",
        "    (200, 421), (421, 18),  (18, 200),  (335, 321), (321, 406), (406, 335),\n",
        "    (321, 320), (320, 405), (405, 321), (314, 315), (315, 17),  (17, 314),\n",
        "    (423, 426), (426, 266), (266, 423), (396, 377), (377, 369), (369, 396),\n",
        "    (270, 322), (322, 269), (269, 270), (413, 417), (417, 464), (464, 413),\n",
        "    (385, 386), (386, 258), (258, 385), (248, 456), (456, 419), (419, 248),\n",
        "    (298, 284), (284, 333), (333, 298), (168, 417), (417, 8),   (8, 168),\n",
        "    (448, 346), (346, 261), (261, 448), (417, 413), (413, 285), (285, 417),\n",
        "    (326, 327), (327, 328), (328, 326), (277, 355), (355, 329), (329, 277),\n",
        "    (309, 392), (392, 438), (438, 309), (381, 382), (382, 256), (256, 381),\n",
        "    (279, 429), (429, 360), (360, 279), (365, 364), (364, 379), (379, 365),\n",
        "    (355, 277), (277, 437), (437, 355), (282, 443), (443, 283), (283, 282),\n",
        "    (281, 275), (275, 363), (363, 281), (395, 431), (431, 369), (369, 395),\n",
        "    (299, 297), (297, 337), (337, 299), (335, 273), (273, 321), (321, 335),\n",
        "    (348, 450), (450, 349), (349, 348), (359, 446), (446, 467), (467, 359),\n",
        "    (283, 293), (293, 282), (282, 283), (250, 458), (458, 462), (462, 250),\n",
        "    (300, 276), (276, 383), (383, 300), (292, 308), (308, 325), (325, 292),\n",
        "    (283, 276), (276, 293), (293, 283), (264, 372), (372, 447), (447, 264),\n",
        "    (346, 352), (352, 340), (340, 346), (354, 274), (274, 19),  (19, 354),\n",
        "    (363, 456), (456, 281), (281, 363), (426, 436), (436, 425), (425, 426),\n",
        "    (380, 381), (381, 252), (252, 380), (267, 269), (269, 393), (393, 267),\n",
        "    (421, 200), (200, 428), (428, 421), (371, 266), (266, 329), (329, 371),\n",
        "    (432, 287), (287, 422), (422, 432), (290, 250), (250, 328), (328, 290),\n",
        "    (385, 258), (258, 384), (384, 385), (446, 265), (265, 342), (342, 446),\n",
        "    (386, 387), (387, 257), (257, 386), (422, 424), (424, 430), (430, 422),\n",
        "    (445, 342), (342, 276), (276, 445), (422, 273), (273, 424), (424, 422),\n",
        "    (306, 292), (292, 307), (307, 306), (352, 366), (366, 345), (345, 352),\n",
        "    (268, 271), (271, 302), (302, 268), (358, 423), (423, 371), (371, 358),\n",
        "    (327, 294), (294, 460), (460, 327), (331, 279), (279, 294), (294, 331),\n",
        "    (303, 271), (271, 304), (304, 303), (436, 432), (432, 427), (427, 436),\n",
        "    (304, 272), (272, 408), (408, 304), (395, 394), (394, 431), (431, 395),\n",
        "    (378, 395), (395, 400), (400, 378), (296, 334), (334, 299), (299, 296),\n",
        "    (6, 351),   (351, 168), (168, 6),   (376, 352), (352, 411), (411, 376),\n",
        "    (307, 325), (325, 320), (320, 307), (285, 295), (295, 336), (336, 285),\n",
        "    (320, 319), (319, 404), (404, 320), (329, 330), (330, 349), (349, 329),\n",
        "    (334, 293), (293, 333), (333, 334), (366, 323), (323, 447), (447, 366),\n",
        "    (316, 15),  (15, 315),  (315, 316), (331, 358), (358, 279), (279, 331),\n",
        "    (317, 14),  (14, 316),  (316, 317), (8, 285),   (285, 9),   (9, 8),\n",
        "    (277, 329), (329, 350), (350, 277), (253, 374), (374, 252), (252, 253),\n",
        "    (319, 318), (318, 403), (403, 319), (351, 6),   (6, 419),   (419, 351),\n",
        "    (324, 318), (318, 325), (325, 324), (397, 367), (367, 365), (365, 397),\n",
        "    (288, 435), (435, 397), (397, 288), (278, 344), (344, 439), (439, 278),\n",
        "    (310, 272), (272, 311), (311, 310), (248, 195), (195, 281), (281, 248),\n",
        "    (375, 273), (273, 291), (291, 375), (175, 396), (396, 199), (199, 175),\n",
        "    (312, 311), (311, 268), (268, 312), (276, 283), (283, 445), (445, 276),\n",
        "    (390, 373), (373, 339), (339, 390), (295, 282), (282, 296), (296, 295),\n",
        "    (448, 449), (449, 346), (346, 448), (356, 264), (264, 454), (454, 356),\n",
        "    (337, 336), (336, 299), (299, 337), (337, 338), (338, 151), (151, 337),\n",
        "    (294, 278), (278, 455), (455, 294), (308, 292), (292, 415), (415, 308),\n",
        "    (429, 358), (358, 355), (355, 429), (265, 340), (340, 372), (372, 265),\n",
        "    (352, 346), (346, 280), (280, 352), (295, 442), (442, 282), (282, 295),\n",
        "    (354, 19),  (19, 370),  (370, 354), (285, 441), (441, 295), (295, 285),\n",
        "    (195, 248), (248, 197), (197, 195), (457, 440), (440, 274), (274, 457),\n",
        "    (301, 300), (300, 368), (368, 301), (417, 351), (351, 465), (465, 417),\n",
        "    (251, 301), (301, 389), (389, 251), (394, 395), (395, 379), (379, 394),\n",
        "    (399, 412), (412, 419), (419, 399), (410, 436), (436, 322), (322, 410),\n",
        "    (326, 2),   (2, 393),   (393, 326), (354, 370), (370, 461), (461, 354),\n",
        "    (393, 164), (164, 267), (267, 393), (268, 302), (302, 12),  (12, 268),\n",
        "    (312, 268), (268, 13),  (13, 312),  (298, 293), (293, 301), (301, 298),\n",
        "    (265, 446), (446, 340), (340, 265), (280, 330), (330, 425), (425, 280),\n",
        "    (322, 426), (426, 391), (391, 322), (420, 429), (429, 437), (437, 420),\n",
        "    (393, 391), (391, 326), (326, 393), (344, 440), (440, 438), (438, 344),\n",
        "    (458, 459), (459, 461), (461, 458), (364, 434), (434, 394), (394, 364),\n",
        "    (428, 396), (396, 262), (262, 428), (274, 354), (354, 457), (457, 274),\n",
        "    (317, 316), (316, 402), (402, 317), (316, 315), (315, 403), (403, 316),\n",
        "    (315, 314), (314, 404), (404, 315), (314, 313), (313, 405), (405, 314),\n",
        "    (313, 421), (421, 406), (406, 313), (323, 366), (366, 361), (361, 323),\n",
        "    (292, 306), (306, 407), (407, 292), (306, 291), (291, 408), (408, 306),\n",
        "    (291, 287), (287, 409), (409, 291), (287, 432), (432, 410), (410, 287),\n",
        "    (427, 434), (434, 411), (411, 427), (372, 264), (264, 383), (383, 372),\n",
        "    (459, 309), (309, 457), (457, 459), (366, 352), (352, 401), (401, 366),\n",
        "    (1, 274),   (274, 4),   (4, 1),     (418, 421), (421, 262), (262, 418),\n",
        "    (331, 294), (294, 358), (358, 331), (435, 433), (433, 367), (367, 435),\n",
        "    (392, 289), (289, 439), (439, 392), (328, 462), (462, 326), (326, 328),\n",
        "    (94, 2),    (2, 370),   (370, 94),  (289, 305), (305, 455), (455, 289),\n",
        "    (339, 254), (254, 448), (448, 339), (359, 255), (255, 446), (446, 359),\n",
        "    (254, 253), (253, 449), (449, 254), (253, 252), (252, 450), (450, 253),\n",
        "    (252, 256), (256, 451), (451, 252), (256, 341), (341, 452), (452, 256),\n",
        "    (414, 413), (413, 463), (463, 414), (286, 441), (441, 414), (414, 286),\n",
        "    (286, 258), (258, 441), (441, 286), (258, 257), (257, 442), (442, 258),\n",
        "    (257, 259), (259, 443), (443, 257), (259, 260), (260, 444), (444, 259),\n",
        "    (260, 467), (467, 445), (445, 260), (309, 459), (459, 250), (250, 309),\n",
        "    (305, 289), (289, 290), (290, 305), (305, 290), (290, 460), (460, 305),\n",
        "    (401, 376), (376, 435), (435, 401), (309, 250), (250, 392), (392, 309),\n",
        "    (376, 411), (411, 433), (433, 376), (453, 341), (341, 464), (464, 453),\n",
        "    (357, 453), (453, 465), (465, 357), (343, 357), (357, 412), (412, 343),\n",
        "    (437, 343), (343, 399), (399, 437), (344, 360), (360, 440), (440, 344),\n",
        "    (420, 437), (437, 456), (456, 420), (360, 420), (420, 363), (363, 360),\n",
        "    (361, 401), (401, 288), (288, 361), (265, 372), (372, 353), (353, 265),\n",
        "    (390, 339), (339, 249), (249, 390), (339, 448), (448, 255), (255, 339)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqozrxXTrPSK"
      },
      "source": [
        "\n",
        "# FACEMESH_CONTOURS =  [[a,b] for a,b in FACEMESH_CONTOURS]\n",
        "# FACEMESH_TESSELATION = [[a,b] for a,b in FACEMESH_TESSELATION]\n",
        "FACEMESH_CONTOURS = list(FACEMESH_CONTOURS)\n",
        "FACEMESH_TESSELATION = list(FACEMESH_TESSELATION)\n",
        "FULL_FACEMESH = set(FACEMESH_CONTOURS + FACEMESH_TESSELATION)\n",
        "FULL_FACEMESH = [[a,b] for a,b in FULL_FACEMESH]\n",
        "# FULL_FACEMESH = [[a,b] for a,b in FACEMESH_CONTOURS]\n",
        "edges = FULL_FACEMESH\n",
        "edges = torch.tensor(edges)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO9lR3eG1Sn2",
        "outputId": "7ccbd61f-b267-4ba3-dfc3-3fe5438c7777"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d88u0qmZHNDn"
      },
      "source": [
        "import pandas as pd\n",
        "face_mesh_train = pd.read_csv('/content/drive/MyDrive/FaceMesh/facemesh_landmarks_AffectNet.csv')\n",
        "face_mesh_test = pd.read_csv('/content/drive/MyDrive/FaceMesh/facemesh_landmarks_AffectNet_val.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9y2j41z_VITv",
        "outputId": "6e7907c7-c8b5-4b34-fdb3-7307f5582bb7"
      },
      "source": [
        "face_mesh_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>expression_label</th>\n",
              "      <th>image_name</th>\n",
              "      <th>x1</th>\n",
              "      <th>y1</th>\n",
              "      <th>z1</th>\n",
              "      <th>x2</th>\n",
              "      <th>y2</th>\n",
              "      <th>z2</th>\n",
              "      <th>x3</th>\n",
              "      <th>y3</th>\n",
              "      <th>z3</th>\n",
              "      <th>x4</th>\n",
              "      <th>y4</th>\n",
              "      <th>z4</th>\n",
              "      <th>x5</th>\n",
              "      <th>y5</th>\n",
              "      <th>z5</th>\n",
              "      <th>x6</th>\n",
              "      <th>y6</th>\n",
              "      <th>z6</th>\n",
              "      <th>x7</th>\n",
              "      <th>y7</th>\n",
              "      <th>z7</th>\n",
              "      <th>x8</th>\n",
              "      <th>y8</th>\n",
              "      <th>z8</th>\n",
              "      <th>x9</th>\n",
              "      <th>y9</th>\n",
              "      <th>z9</th>\n",
              "      <th>x10</th>\n",
              "      <th>y10</th>\n",
              "      <th>z10</th>\n",
              "      <th>x11</th>\n",
              "      <th>y11</th>\n",
              "      <th>z11</th>\n",
              "      <th>x12</th>\n",
              "      <th>y12</th>\n",
              "      <th>z12</th>\n",
              "      <th>x13</th>\n",
              "      <th>y13</th>\n",
              "      <th>...</th>\n",
              "      <th>z455</th>\n",
              "      <th>x456</th>\n",
              "      <th>y456</th>\n",
              "      <th>z456</th>\n",
              "      <th>x457</th>\n",
              "      <th>y457</th>\n",
              "      <th>z457</th>\n",
              "      <th>x458</th>\n",
              "      <th>y458</th>\n",
              "      <th>z458</th>\n",
              "      <th>x459</th>\n",
              "      <th>y459</th>\n",
              "      <th>z459</th>\n",
              "      <th>x460</th>\n",
              "      <th>y460</th>\n",
              "      <th>z460</th>\n",
              "      <th>x461</th>\n",
              "      <th>y461</th>\n",
              "      <th>z461</th>\n",
              "      <th>x462</th>\n",
              "      <th>y462</th>\n",
              "      <th>z462</th>\n",
              "      <th>x463</th>\n",
              "      <th>y463</th>\n",
              "      <th>z463</th>\n",
              "      <th>x464</th>\n",
              "      <th>y464</th>\n",
              "      <th>z464</th>\n",
              "      <th>x465</th>\n",
              "      <th>y465</th>\n",
              "      <th>z465</th>\n",
              "      <th>x466</th>\n",
              "      <th>y466</th>\n",
              "      <th>z466</th>\n",
              "      <th>x467</th>\n",
              "      <th>y467</th>\n",
              "      <th>z467</th>\n",
              "      <th>x468</th>\n",
              "      <th>y468</th>\n",
              "      <th>z468</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>362777.jpg</td>\n",
              "      <td>0.484722</td>\n",
              "      <td>0.674200</td>\n",
              "      <td>-0.103766</td>\n",
              "      <td>0.498985</td>\n",
              "      <td>0.568210</td>\n",
              "      <td>-0.230872</td>\n",
              "      <td>0.491299</td>\n",
              "      <td>0.600221</td>\n",
              "      <td>-0.120523</td>\n",
              "      <td>0.463147</td>\n",
              "      <td>0.446264</td>\n",
              "      <td>-0.184573</td>\n",
              "      <td>0.501151</td>\n",
              "      <td>0.531049</td>\n",
              "      <td>-0.247626</td>\n",
              "      <td>0.501428</td>\n",
              "      <td>0.482070</td>\n",
              "      <td>-0.233998</td>\n",
              "      <td>0.499302</td>\n",
              "      <td>0.365151</td>\n",
              "      <td>-0.128537</td>\n",
              "      <td>0.181390</td>\n",
              "      <td>0.349469</td>\n",
              "      <td>0.024498</td>\n",
              "      <td>0.499953</td>\n",
              "      <td>0.272851</td>\n",
              "      <td>-0.103315</td>\n",
              "      <td>0.501786</td>\n",
              "      <td>0.221700</td>\n",
              "      <td>-0.115673</td>\n",
              "      <td>0.505607</td>\n",
              "      <td>0.038215</td>\n",
              "      <td>-0.089334</td>\n",
              "      <td>0.483321</td>\n",
              "      <td>0.690962</td>\n",
              "      <td>-0.096040</td>\n",
              "      <td>0.481734</td>\n",
              "      <td>0.701384</td>\n",
              "      <td>...</td>\n",
              "      <td>0.462577</td>\n",
              "      <td>0.623592</td>\n",
              "      <td>0.570202</td>\n",
              "      <td>-0.100558</td>\n",
              "      <td>0.567662</td>\n",
              "      <td>0.465538</td>\n",
              "      <td>-0.146924</td>\n",
              "      <td>0.564230</td>\n",
              "      <td>0.562064</td>\n",
              "      <td>-0.204385</td>\n",
              "      <td>0.538077</td>\n",
              "      <td>0.578656</td>\n",
              "      <td>-0.186902</td>\n",
              "      <td>0.563540</td>\n",
              "      <td>0.566732</td>\n",
              "      <td>-0.183534</td>\n",
              "      <td>0.611722</td>\n",
              "      <td>0.579660</td>\n",
              "      <td>-0.086172</td>\n",
              "      <td>0.530709</td>\n",
              "      <td>0.582517</td>\n",
              "      <td>-0.198298</td>\n",
              "      <td>0.525455</td>\n",
              "      <td>0.588519</td>\n",
              "      <td>-0.147508</td>\n",
              "      <td>0.600449</td>\n",
              "      <td>0.368875</td>\n",
              "      <td>0.016978</td>\n",
              "      <td>0.577954</td>\n",
              "      <td>0.372717</td>\n",
              "      <td>-0.009346</td>\n",
              "      <td>0.566348</td>\n",
              "      <td>0.375940</td>\n",
              "      <td>-0.039107</td>\n",
              "      <td>0.793190</td>\n",
              "      <td>0.358794</td>\n",
              "      <td>0.053452</td>\n",
              "      <td>0.814010</td>\n",
              "      <td>0.343849</td>\n",
              "      <td>0.056747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>310468.jpg</td>\n",
              "      <td>0.494335</td>\n",
              "      <td>0.697882</td>\n",
              "      <td>-0.099114</td>\n",
              "      <td>0.493664</td>\n",
              "      <td>0.575031</td>\n",
              "      <td>-0.189239</td>\n",
              "      <td>0.496904</td>\n",
              "      <td>0.612648</td>\n",
              "      <td>-0.098116</td>\n",
              "      <td>0.470560</td>\n",
              "      <td>0.461784</td>\n",
              "      <td>-0.139983</td>\n",
              "      <td>0.493982</td>\n",
              "      <td>0.539344</td>\n",
              "      <td>-0.201322</td>\n",
              "      <td>0.496368</td>\n",
              "      <td>0.493422</td>\n",
              "      <td>-0.187634</td>\n",
              "      <td>0.504072</td>\n",
              "      <td>0.385410</td>\n",
              "      <td>-0.094126</td>\n",
              "      <td>0.269798</td>\n",
              "      <td>0.378828</td>\n",
              "      <td>0.064243</td>\n",
              "      <td>0.506757</td>\n",
              "      <td>0.318395</td>\n",
              "      <td>-0.072008</td>\n",
              "      <td>0.507349</td>\n",
              "      <td>0.275220</td>\n",
              "      <td>-0.080529</td>\n",
              "      <td>0.511927</td>\n",
              "      <td>0.093088</td>\n",
              "      <td>-0.039707</td>\n",
              "      <td>0.494362</td>\n",
              "      <td>0.714882</td>\n",
              "      <td>-0.094221</td>\n",
              "      <td>0.495025</td>\n",
              "      <td>0.725922</td>\n",
              "      <td>...</td>\n",
              "      <td>0.356179</td>\n",
              "      <td>0.597822</td>\n",
              "      <td>0.589248</td>\n",
              "      <td>-0.087350</td>\n",
              "      <td>0.556255</td>\n",
              "      <td>0.479199</td>\n",
              "      <td>-0.119355</td>\n",
              "      <td>0.548014</td>\n",
              "      <td>0.570020</td>\n",
              "      <td>-0.170760</td>\n",
              "      <td>0.529288</td>\n",
              "      <td>0.586602</td>\n",
              "      <td>-0.155316</td>\n",
              "      <td>0.548476</td>\n",
              "      <td>0.575814</td>\n",
              "      <td>-0.153296</td>\n",
              "      <td>0.587786</td>\n",
              "      <td>0.599722</td>\n",
              "      <td>-0.074993</td>\n",
              "      <td>0.522674</td>\n",
              "      <td>0.589551</td>\n",
              "      <td>-0.164815</td>\n",
              "      <td>0.520467</td>\n",
              "      <td>0.597360</td>\n",
              "      <td>-0.121952</td>\n",
              "      <td>0.598955</td>\n",
              "      <td>0.381840</td>\n",
              "      <td>0.020255</td>\n",
              "      <td>0.577145</td>\n",
              "      <td>0.389441</td>\n",
              "      <td>-0.000505</td>\n",
              "      <td>0.565155</td>\n",
              "      <td>0.394212</td>\n",
              "      <td>-0.026449</td>\n",
              "      <td>0.763206</td>\n",
              "      <td>0.365569</td>\n",
              "      <td>0.033023</td>\n",
              "      <td>0.779797</td>\n",
              "      <td>0.360694</td>\n",
              "      <td>0.032383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>142530.jpg</td>\n",
              "      <td>0.512581</td>\n",
              "      <td>0.650553</td>\n",
              "      <td>-0.152598</td>\n",
              "      <td>0.494950</td>\n",
              "      <td>0.528617</td>\n",
              "      <td>-0.206429</td>\n",
              "      <td>0.501104</td>\n",
              "      <td>0.575894</td>\n",
              "      <td>-0.133786</td>\n",
              "      <td>0.455063</td>\n",
              "      <td>0.434584</td>\n",
              "      <td>-0.125698</td>\n",
              "      <td>0.490977</td>\n",
              "      <td>0.493492</td>\n",
              "      <td>-0.208501</td>\n",
              "      <td>0.486430</td>\n",
              "      <td>0.454838</td>\n",
              "      <td>-0.181293</td>\n",
              "      <td>0.476521</td>\n",
              "      <td>0.368148</td>\n",
              "      <td>-0.049063</td>\n",
              "      <td>0.235854</td>\n",
              "      <td>0.406111</td>\n",
              "      <td>0.085123</td>\n",
              "      <td>0.468804</td>\n",
              "      <td>0.298283</td>\n",
              "      <td>0.002723</td>\n",
              "      <td>0.464436</td>\n",
              "      <td>0.255713</td>\n",
              "      <td>0.014200</td>\n",
              "      <td>0.448834</td>\n",
              "      <td>0.110829</td>\n",
              "      <td>0.112475</td>\n",
              "      <td>0.514758</td>\n",
              "      <td>0.667027</td>\n",
              "      <td>-0.153776</td>\n",
              "      <td>0.516510</td>\n",
              "      <td>0.681888</td>\n",
              "      <td>...</td>\n",
              "      <td>0.368067</td>\n",
              "      <td>0.596656</td>\n",
              "      <td>0.545409</td>\n",
              "      <td>-0.108755</td>\n",
              "      <td>0.540953</td>\n",
              "      <td>0.445370</td>\n",
              "      <td>-0.105333</td>\n",
              "      <td>0.546755</td>\n",
              "      <td>0.522139</td>\n",
              "      <td>-0.183702</td>\n",
              "      <td>0.530848</td>\n",
              "      <td>0.543908</td>\n",
              "      <td>-0.176569</td>\n",
              "      <td>0.547823</td>\n",
              "      <td>0.530305</td>\n",
              "      <td>-0.168531</td>\n",
              "      <td>0.589756</td>\n",
              "      <td>0.558313</td>\n",
              "      <td>-0.100492</td>\n",
              "      <td>0.524989</td>\n",
              "      <td>0.546166</td>\n",
              "      <td>-0.186621</td>\n",
              "      <td>0.523402</td>\n",
              "      <td>0.557360</td>\n",
              "      <td>-0.150215</td>\n",
              "      <td>0.561529</td>\n",
              "      <td>0.368643</td>\n",
              "      <td>0.069732</td>\n",
              "      <td>0.545015</td>\n",
              "      <td>0.376370</td>\n",
              "      <td>0.044947</td>\n",
              "      <td>0.535405</td>\n",
              "      <td>0.379154</td>\n",
              "      <td>0.015779</td>\n",
              "      <td>0.716662</td>\n",
              "      <td>0.348989</td>\n",
              "      <td>0.094529</td>\n",
              "      <td>0.737134</td>\n",
              "      <td>0.332964</td>\n",
              "      <td>0.097806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>333865.jpg</td>\n",
              "      <td>0.563293</td>\n",
              "      <td>0.729145</td>\n",
              "      <td>-0.082494</td>\n",
              "      <td>0.575636</td>\n",
              "      <td>0.619196</td>\n",
              "      <td>-0.180636</td>\n",
              "      <td>0.557811</td>\n",
              "      <td>0.648760</td>\n",
              "      <td>-0.086905</td>\n",
              "      <td>0.523952</td>\n",
              "      <td>0.498928</td>\n",
              "      <td>-0.153360</td>\n",
              "      <td>0.575247</td>\n",
              "      <td>0.583711</td>\n",
              "      <td>-0.195534</td>\n",
              "      <td>0.565870</td>\n",
              "      <td>0.533678</td>\n",
              "      <td>-0.186283</td>\n",
              "      <td>0.534254</td>\n",
              "      <td>0.406923</td>\n",
              "      <td>-0.108189</td>\n",
              "      <td>0.250968</td>\n",
              "      <td>0.401127</td>\n",
              "      <td>-0.025589</td>\n",
              "      <td>0.522462</td>\n",
              "      <td>0.322320</td>\n",
              "      <td>-0.095484</td>\n",
              "      <td>0.520622</td>\n",
              "      <td>0.276047</td>\n",
              "      <td>-0.109184</td>\n",
              "      <td>0.498536</td>\n",
              "      <td>0.082960</td>\n",
              "      <td>-0.090516</td>\n",
              "      <td>0.563605</td>\n",
              "      <td>0.746594</td>\n",
              "      <td>-0.075727</td>\n",
              "      <td>0.561136</td>\n",
              "      <td>0.759066</td>\n",
              "      <td>...</td>\n",
              "      <td>0.448930</td>\n",
              "      <td>0.642369</td>\n",
              "      <td>0.609373</td>\n",
              "      <td>-0.058047</td>\n",
              "      <td>0.601179</td>\n",
              "      <td>0.503203</td>\n",
              "      <td>-0.106503</td>\n",
              "      <td>0.619107</td>\n",
              "      <td>0.604945</td>\n",
              "      <td>-0.150851</td>\n",
              "      <td>0.598233</td>\n",
              "      <td>0.622277</td>\n",
              "      <td>-0.139377</td>\n",
              "      <td>0.614971</td>\n",
              "      <td>0.608726</td>\n",
              "      <td>-0.133058</td>\n",
              "      <td>0.631975</td>\n",
              "      <td>0.620719</td>\n",
              "      <td>-0.048166</td>\n",
              "      <td>0.595054</td>\n",
              "      <td>0.627434</td>\n",
              "      <td>-0.149354</td>\n",
              "      <td>0.587006</td>\n",
              "      <td>0.633145</td>\n",
              "      <td>-0.107513</td>\n",
              "      <td>0.588086</td>\n",
              "      <td>0.375746</td>\n",
              "      <td>0.028189</td>\n",
              "      <td>0.575098</td>\n",
              "      <td>0.389331</td>\n",
              "      <td>0.001591</td>\n",
              "      <td>0.572228</td>\n",
              "      <td>0.399200</td>\n",
              "      <td>-0.026210</td>\n",
              "      <td>0.731154</td>\n",
              "      <td>0.332842</td>\n",
              "      <td>0.080987</td>\n",
              "      <td>0.744518</td>\n",
              "      <td>0.324053</td>\n",
              "      <td>0.084082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>197416.jpg</td>\n",
              "      <td>0.584781</td>\n",
              "      <td>0.637844</td>\n",
              "      <td>-0.125516</td>\n",
              "      <td>0.578704</td>\n",
              "      <td>0.521914</td>\n",
              "      <td>-0.204803</td>\n",
              "      <td>0.573109</td>\n",
              "      <td>0.566421</td>\n",
              "      <td>-0.119920</td>\n",
              "      <td>0.518106</td>\n",
              "      <td>0.421876</td>\n",
              "      <td>-0.145650</td>\n",
              "      <td>0.572385</td>\n",
              "      <td>0.484992</td>\n",
              "      <td>-0.212535</td>\n",
              "      <td>0.559440</td>\n",
              "      <td>0.442628</td>\n",
              "      <td>-0.191979</td>\n",
              "      <td>0.524140</td>\n",
              "      <td>0.348500</td>\n",
              "      <td>-0.077384</td>\n",
              "      <td>0.251479</td>\n",
              "      <td>0.403486</td>\n",
              "      <td>0.029593</td>\n",
              "      <td>0.506633</td>\n",
              "      <td>0.286570</td>\n",
              "      <td>-0.040093</td>\n",
              "      <td>0.498650</td>\n",
              "      <td>0.243280</td>\n",
              "      <td>-0.039382</td>\n",
              "      <td>0.456619</td>\n",
              "      <td>0.070168</td>\n",
              "      <td>0.027771</td>\n",
              "      <td>0.587028</td>\n",
              "      <td>0.655114</td>\n",
              "      <td>-0.123156</td>\n",
              "      <td>0.587322</td>\n",
              "      <td>0.667975</td>\n",
              "      <td>...</td>\n",
              "      <td>0.454515</td>\n",
              "      <td>0.663650</td>\n",
              "      <td>0.524981</td>\n",
              "      <td>-0.085376</td>\n",
              "      <td>0.601179</td>\n",
              "      <td>0.427280</td>\n",
              "      <td>-0.105366</td>\n",
              "      <td>0.625010</td>\n",
              "      <td>0.510604</td>\n",
              "      <td>-0.174567</td>\n",
              "      <td>0.606597</td>\n",
              "      <td>0.533907</td>\n",
              "      <td>-0.166719</td>\n",
              "      <td>0.623176</td>\n",
              "      <td>0.518920</td>\n",
              "      <td>-0.157562</td>\n",
              "      <td>0.655589</td>\n",
              "      <td>0.537390</td>\n",
              "      <td>-0.077035</td>\n",
              "      <td>0.603331</td>\n",
              "      <td>0.536717</td>\n",
              "      <td>-0.177540</td>\n",
              "      <td>0.597945</td>\n",
              "      <td>0.547499</td>\n",
              "      <td>-0.137365</td>\n",
              "      <td>0.592695</td>\n",
              "      <td>0.346796</td>\n",
              "      <td>0.065316</td>\n",
              "      <td>0.577675</td>\n",
              "      <td>0.354520</td>\n",
              "      <td>0.036788</td>\n",
              "      <td>0.571957</td>\n",
              "      <td>0.357606</td>\n",
              "      <td>0.005771</td>\n",
              "      <td>0.739752</td>\n",
              "      <td>0.313402</td>\n",
              "      <td>0.114589</td>\n",
              "      <td>0.755124</td>\n",
              "      <td>0.302153</td>\n",
              "      <td>0.118926</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1406 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   expression_label  image_name        x1  ...      x468      y468      z468\n",
              "0                 1  362777.jpg  0.484722  ...  0.814010  0.343849  0.056747\n",
              "1                 0  310468.jpg  0.494335  ...  0.779797  0.360694  0.032383\n",
              "2                 3  142530.jpg  0.512581  ...  0.737134  0.332964  0.097806\n",
              "3                 6  333865.jpg  0.563293  ...  0.744518  0.324053  0.084082\n",
              "4                 1  197416.jpg  0.584781  ...  0.755124  0.302153  0.118926\n",
              "\n",
              "[5 rows x 1406 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6YOzIbcV5C3"
      },
      "source": [
        "# row = face_mesh_train.iloc[0]\n",
        "# x,y,z =None, None, None\n",
        "# for j in range(0, len(row[2:])-2, 3):\n",
        "#   x,y,z = row[2+j],row[3+j],row[4+j]\n",
        "#   # fm_data.append([x,y,z])\n",
        "# print(x,y,z)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0oRbeEoHTiS"
      },
      "source": [
        "data_set_train = []\n",
        "data_set_test = []\n",
        "\n",
        "for i in range(face_mesh_train.shape[0]):\n",
        "  row = face_mesh_train.iloc[i]\n",
        "  # print(row)\n",
        "  # break\n",
        "  fm_data = []\n",
        "  x,y,z = None, None, None\n",
        "  for j in range(0, len(row[2:])-2, 3):\n",
        "    x,y,z = row[2+j],row[3+j],row[4+j]\n",
        "    fm_data.append([x,y,z])\n",
        "    # print(x,y,z)\n",
        "    # break\n",
        "  fm_data = torch.tensor(fm_data)\n",
        "  y = torch.tensor([row[0]])\n",
        "  data_fm = Data(x=fm_data,edge_index=edges.t(), y = y)\n",
        "  data_set_train.append(data_fm)\n",
        "  \n",
        "  \n",
        "for i in range(face_mesh_test.shape[0]):\n",
        "  row = face_mesh_test.iloc[i]\n",
        "  # print(row)\n",
        "  # break\n",
        "  fm_data = []\n",
        "  x,y,z = None, None, None\n",
        "  for j in range(0, len(row[2:])-2, 3):\n",
        "    x,y,z = row[2+j],row[3+j],row[4+j]\n",
        "    fm_data.append([x,y,z])\n",
        "    # print(x,y,z)\n",
        "    # break\n",
        "  fm_data = torch.tensor(fm_data)\n",
        "  y = torch.tensor([row[0]])\n",
        "  data_fm = Data(x=fm_data,edge_index=edges.t(), y = y)\n",
        "  data_set_test.append(data_fm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAVWL01H2v5W"
      },
      "source": [
        "batch_size = 64\n",
        "loader_test = DataLoader(data_set_test, batch_size=batch_size)\n",
        "loader_train = DataLoader(data_set_train, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRLjlD2_wdD5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76925a05-d20d-4752-8506-644fa8d100ff"
      },
      "source": [
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F \n",
        "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool\n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "from torch_geometric.nn import BatchNorm\n",
        "num_node_features = 3\n",
        "num_classes = len(face_mesh_train.expression_label.unique())\n",
        "embedding_size = 64\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        # Init parent\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(42)\n",
        "\n",
        "        # GCN layers\n",
        "        self.initial_conv = GCNConv(num_node_features, hidden_channels)\n",
        "        self.bn1 = BatchNorm(hidden_channels)\n",
        "        self.conv1 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.bn2 = BatchNorm(hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.bn3 = BatchNorm(hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.bn4 = BatchNorm(hidden_channels)\n",
        "        \n",
        "       \n",
        "        # Output layer\n",
        "        self.out = Linear(hidden_channels*2, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch_index):\n",
        "        # First Conv layer\n",
        "        hidden = self.initial_conv(x, edge_index)\n",
        "        hidden = self.bn1(hidden)\n",
        "        hidden = torch.tanh(hidden)\n",
        "\n",
        "        # Other Conv layers\n",
        "        hidden = self.conv1(hidden, edge_index)\n",
        "        hidden = self.bn2(hidden)\n",
        "        hidden = torch.tanh(hidden)\n",
        "        hidden = self.conv2(hidden, edge_index)\n",
        "        hidden = self.bn3(hidden)\n",
        "        hidden = torch.tanh(hidden)\n",
        "        hidden = self.conv3(hidden, edge_index)\n",
        "        hidden = self.bn4(hidden)\n",
        "        hidden = torch.tanh(hidden)\n",
        "          \n",
        "        # Global Pooling (stack different aggregations)\n",
        "        hidden = torch.cat([gmp(hidden, batch_index), \n",
        "                            gap(hidden, batch_index)], dim=1)\n",
        "        \n",
        "        hidden = F.dropout(hidden, p=0.3, training=self.training)\n",
        "\n",
        "        # Apply a final (linear) classifier.\n",
        "        out = self.out(hidden)\n",
        "\n",
        "        return out\n",
        "\n",
        "hidden_channels = 32\n",
        "model = GCN(hidden_channels=hidden_channels)\n",
        "print(model)\n",
        "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))\n",
        "print(next(model.parameters()).is_cuda)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (initial_conv): GCNConv(3, 32)\n",
            "  (bn1): BatchNorm(32)\n",
            "  (conv1): GCNConv(32, 32)\n",
            "  (bn2): BatchNorm(32)\n",
            "  (conv2): GCNConv(32, 32)\n",
            "  (bn3): BatchNorm(32)\n",
            "  (conv3): GCNConv(32, 32)\n",
            "  (bn4): BatchNorm(32)\n",
            "  (out): Linear(in_features=64, out_features=8, bias=True)\n",
            ")\n",
            "Number of parameters:  4072\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJlNm1sPKsI9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea6123c5-c75c-4ec9-b604-217935482445"
      },
      "source": [
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F \n",
        "from torch_geometric.nn import GCNConv,GraphConv, TopKPooling, global_mean_pool\n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "from torch_geometric.nn import BatchNorm\n",
        "num_node_features = 3\n",
        "num_classes = len(face_mesh_train.expression_label.unique())\n",
        "embedding_size = 64\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        # Init parent\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(42)\n",
        "\n",
        "        # GCN layers\n",
        "        self.initial_conv = GraphConv(num_node_features, hidden_channels)\n",
        "        # self.bn1 = BatchNorm(hidden_channels)\n",
        "        self.conv1 = GraphConv(hidden_channels, hidden_channels)\n",
        "        # self.bn2 = BatchNorm(hidden_channels)\n",
        "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
        "        # self.bn3 = BatchNorm(hidden_channels)\n",
        "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
        "        # self.bn4 = BatchNorm(hidden_channels)\n",
        "        \n",
        "       \n",
        "        # Output layer\n",
        "        self.out = Linear(hidden_channels*2, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch_index):\n",
        "        # First Conv layer\n",
        "        hidden = self.initial_conv(x, edge_index)\n",
        "        # hidden = self.bn1(hidden)\n",
        "        hidden = torch.tanh(hidden)\n",
        "\n",
        "        # Other Conv layers\n",
        "        hidden = self.conv1(hidden, edge_index)\n",
        "        # hidden = self.bn2(hidden)\n",
        "        hidden = torch.tanh(hidden)\n",
        "        hidden = self.conv2(hidden, edge_index)\n",
        "        # hidden = self.bn3(hidden)\n",
        "        hidden = torch.tanh(hidden)\n",
        "        hidden = self.conv3(hidden, edge_index)\n",
        "        # hidden = self.bn4(hidden)\n",
        "        hidden = torch.tanh(hidden)\n",
        "          \n",
        "        # Global Pooling (stack different aggregations)\n",
        "        hidden = torch.cat([gmp(hidden, batch_index), \n",
        "                            gap(hidden, batch_index)], dim=1)\n",
        "        \n",
        "        # hidden = F.dropout(hidden, p=0.3, training=self.training)\n",
        "\n",
        "        # Apply a final (linear) classifier.\n",
        "        out = self.out(hidden)\n",
        "\n",
        "        return out\n",
        "\n",
        "hidden_channels = 32\n",
        "model = GCN(hidden_channels=hidden_channels)\n",
        "print(model)\n",
        "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))\n",
        "print(next(model.parameters()).is_cuda)\n",
        "# https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing#scrollTo=cNgkR8SRaU_P"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (initial_conv): GraphConv(3, 32)\n",
            "  (conv1): GraphConv(32, 32)\n",
            "  (conv2): GraphConv(32, 32)\n",
            "  (conv3): GraphConv(32, 32)\n",
            "  (out): Linear(in_features=64, out_features=8, bias=True)\n",
            ")\n",
            "Number of parameters:  6984\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqTz-gwmdCTA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50f39af6-9ac1-4aaa-af0d-98301db47f20"
      },
      "source": [
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F \n",
        "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool\n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "num_node_features = 3\n",
        "num_classes = len(face_mesh_train.expression_label.unique())\n",
        "embedding_size = 64\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        # Init parent\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(42)\n",
        "\n",
        "        # GCN layers\n",
        "        self.initial_conv = GCNConv(num_node_features, hidden_channels)\n",
        "        self.conv1 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
        "\n",
        "        # Output layer\n",
        "        self.out = Linear(hidden_channels*2, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch_index):\n",
        "        # First Conv layer\n",
        "        hidden = self.initial_conv(x, edge_index)\n",
        "        hidden = F.tanh(hidden)\n",
        "\n",
        "        # Other Conv layers\n",
        "        hidden = self.conv1(hidden, edge_index)\n",
        "        hidden = F.tanh(hidden)\n",
        "        hidden = self.conv2(hidden, edge_index)\n",
        "        hidden = F.tanh(hidden)\n",
        "        hidden = self.conv3(hidden, edge_index)\n",
        "        hidden = F.tanh(hidden)\n",
        "        hidden = self.conv4(hidden, edge_index)\n",
        "        hidden = F.tanh(hidden)\n",
        "        \n",
        "        # Global Pooling (stack different aggregations)\n",
        "        hidden = torch.cat([gmp(hidden, batch_index), \n",
        "                            gap(hidden, batch_index)], dim=1)\n",
        "\n",
        "        # Apply a final (linear) classifier.\n",
        "        out = self.out(hidden)\n",
        "\n",
        "        return out\n",
        "\n",
        "hidden_channels = 32\n",
        "model = GCN(hidden_channels=hidden_channels)\n",
        "print(model)\n",
        "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))\n",
        "print(next(model.parameters()).is_cuda)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (initial_conv): GCNConv(3, 32)\n",
            "  (conv1): GCNConv(32, 32)\n",
            "  (conv2): GCNConv(32, 32)\n",
            "  (conv3): GCNConv(32, 32)\n",
            "  (conv4): GCNConv(32, 32)\n",
            "  (out): Linear(in_features=64, out_features=8, bias=True)\n",
            ")\n",
            "Number of parameters:  4872\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2OUXUbk3cnY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "4e8394ba-1992-42cf-d746-a5e33da63950"
      },
      "source": [
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = GCN(hidden_channels=hidden_channels)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=.0007)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model = model):\n",
        "    #print(optimizer)\n",
        "    model.train()\n",
        "\n",
        "    for data in loader_train:  # Iterate in batches over the training dataset.\n",
        "         data.to(device)\n",
        "         out = model(data.x.float(), data.edge_index, data.batch)  # Perform a single forward pass.\n",
        "         loss = criterion(out, data.y)  # Compute the loss.\n",
        "         loss.backward()  # Derive gradients.\n",
        "         optimizer.step()  # Update parameters based on gradients.\n",
        "         optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "def test(loader, model = model):\n",
        "     model.eval()\n",
        "     running_loss = 0\n",
        "     correct = 0\n",
        "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "         data.to(device) \n",
        "         out = model(data.x.float(), data.edge_index, data.batch)  \n",
        "         batch_size = data.y.shape[0]\n",
        "         loss = criterion(out, data.y)\n",
        "         running_loss += (loss.item() *batch_size)\n",
        "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
        "    #  print(len(loader.dataset))\n",
        "     return (running_loss/len(loader.dataset)), correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRXqHNuE4I-U"
      },
      "source": [
        "def tune(PATH, model = model, log_interval = 20):\n",
        "  for epoch in range(1, 4000):\n",
        "      train(model)\n",
        "      train_loss, train_acc = test(loader_train, model)\n",
        "      test_loss, test_acc = test(loader_test, model)\n",
        "      if epoch%log_interval==0:\n",
        "        torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'loss':test_loss,\n",
        "              'acc':test_acc\n",
        "              }, PATH) \n",
        "        print(f'Epoch: {epoch:03d}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f} Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj9EIYRI11Kd"
      },
      "source": [
        "def fine_tune(PATH, SAVE_PATH, lr, log_interval = 20):\n",
        "  global optimizer\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model = GCN(hidden_channels=hidden_channels)\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  checkpoint = torch.load(PATH, map_location =device)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  epoch = checkpoint['epoch']\n",
        "  train_loss, train_acc = test(loader_train, model)\n",
        "  test_loss, test_acc = test(loader_test, model)\n",
        "  print(f'Epoch: {epoch:03d}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f} Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
        "  tune(SAVE_PATH, model, log_interval)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCchRaLHLRE4"
      },
      "source": [
        "base_path = '/content/drive/MyDrive/FaceMesh'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKO7jkjF3J5F"
      },
      "source": [
        "## Tune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ku-KS0Mx3FUc",
        "outputId": "dc669507-8936-4676-af1c-a848c8594012"
      },
      "source": [
        "SAVE_PATH = f\"{base_path}/model_first.bin\"\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = GCN(hidden_channels=hidden_channels)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=.01)\n",
        "tune(SAVE_PATH, model, log_interval=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train loss: 1.2144, Test loss: 2.7144 Train Acc: 0.5931, Test Acc: 0.1971\n",
            "Epoch: 002, Train loss: 1.2001, Test loss: 2.6390 Train Acc: 0.6022, Test Acc: 0.2101\n",
            "Epoch: 003, Train loss: 1.2343, Test loss: 2.5907 Train Acc: 0.5889, Test Acc: 0.2172\n",
            "Epoch: 004, Train loss: 1.2071, Test loss: 2.5688 Train Acc: 0.5961, Test Acc: 0.2204\n",
            "Epoch: 005, Train loss: 1.2045, Test loss: 2.6587 Train Acc: 0.6028, Test Acc: 0.2182\n",
            "Epoch: 006, Train loss: 1.1900, Test loss: 2.5812 Train Acc: 0.6034, Test Acc: 0.2207\n",
            "Epoch: 007, Train loss: 1.1818, Test loss: 2.5492 Train Acc: 0.6061, Test Acc: 0.2229\n",
            "Epoch: 008, Train loss: 1.2035, Test loss: 2.5928 Train Acc: 0.6039, Test Acc: 0.2229\n",
            "Epoch: 009, Train loss: 1.1728, Test loss: 2.5391 Train Acc: 0.6125, Test Acc: 0.2247\n",
            "Epoch: 010, Train loss: 1.1797, Test loss: 2.5693 Train Acc: 0.6070, Test Acc: 0.2252\n",
            "Epoch: 011, Train loss: 1.1906, Test loss: 2.5528 Train Acc: 0.6058, Test Acc: 0.2315\n",
            "Epoch: 012, Train loss: 1.1647, Test loss: 2.5715 Train Acc: 0.6151, Test Acc: 0.2287\n",
            "Epoch: 013, Train loss: 1.1829, Test loss: 2.5828 Train Acc: 0.6096, Test Acc: 0.2285\n",
            "Epoch: 014, Train loss: 1.1718, Test loss: 2.6297 Train Acc: 0.6107, Test Acc: 0.2252\n",
            "Epoch: 015, Train loss: 1.1860, Test loss: 2.6125 Train Acc: 0.6066, Test Acc: 0.2312\n",
            "Epoch: 016, Train loss: 1.1835, Test loss: 2.5841 Train Acc: 0.6053, Test Acc: 0.2267\n",
            "Epoch: 017, Train loss: 1.1813, Test loss: 2.5823 Train Acc: 0.6074, Test Acc: 0.2280\n",
            "Epoch: 018, Train loss: 1.1722, Test loss: 2.5871 Train Acc: 0.6121, Test Acc: 0.2300\n",
            "Epoch: 019, Train loss: 1.1832, Test loss: 2.5795 Train Acc: 0.6059, Test Acc: 0.2275\n",
            "Epoch: 020, Train loss: 1.1773, Test loss: 2.5973 Train Acc: 0.6089, Test Acc: 0.2282\n",
            "Epoch: 021, Train loss: 1.1650, Test loss: 2.5638 Train Acc: 0.6123, Test Acc: 0.2302\n",
            "Epoch: 022, Train loss: 1.1945, Test loss: 2.5574 Train Acc: 0.6018, Test Acc: 0.2285\n",
            "Epoch: 023, Train loss: 1.1465, Test loss: 2.5585 Train Acc: 0.6189, Test Acc: 0.2267\n",
            "Epoch: 024, Train loss: 1.1555, Test loss: 2.5343 Train Acc: 0.6166, Test Acc: 0.2277\n",
            "Epoch: 025, Train loss: 1.1498, Test loss: 2.5497 Train Acc: 0.6162, Test Acc: 0.2282\n",
            "Epoch: 026, Train loss: 1.1508, Test loss: 2.5778 Train Acc: 0.6161, Test Acc: 0.2275\n",
            "Epoch: 027, Train loss: 1.1669, Test loss: 2.5668 Train Acc: 0.6114, Test Acc: 0.2302\n",
            "Epoch: 028, Train loss: 1.1548, Test loss: 2.5771 Train Acc: 0.6154, Test Acc: 0.2272\n",
            "Epoch: 029, Train loss: 1.1512, Test loss: 2.5937 Train Acc: 0.6175, Test Acc: 0.2307\n",
            "Epoch: 030, Train loss: 1.1831, Test loss: 2.5476 Train Acc: 0.6054, Test Acc: 0.2352\n",
            "Epoch: 031, Train loss: 1.1617, Test loss: 2.5592 Train Acc: 0.6121, Test Acc: 0.2277\n",
            "Epoch: 032, Train loss: 1.1530, Test loss: 2.5577 Train Acc: 0.6163, Test Acc: 0.2332\n",
            "Epoch: 033, Train loss: 1.1359, Test loss: 2.5910 Train Acc: 0.6220, Test Acc: 0.2315\n",
            "Epoch: 034, Train loss: 1.1409, Test loss: 2.5614 Train Acc: 0.6201, Test Acc: 0.2297\n",
            "Epoch: 035, Train loss: 1.1530, Test loss: 2.5508 Train Acc: 0.6170, Test Acc: 0.2360\n",
            "Epoch: 036, Train loss: 1.1433, Test loss: 2.5758 Train Acc: 0.6192, Test Acc: 0.2368\n",
            "Epoch: 037, Train loss: 1.1415, Test loss: 2.5914 Train Acc: 0.6216, Test Acc: 0.2322\n",
            "Epoch: 038, Train loss: 1.1630, Test loss: 2.5985 Train Acc: 0.6110, Test Acc: 0.2330\n",
            "Epoch: 039, Train loss: 1.1303, Test loss: 2.5805 Train Acc: 0.6224, Test Acc: 0.2320\n",
            "Epoch: 040, Train loss: 1.1274, Test loss: 2.5910 Train Acc: 0.6233, Test Acc: 0.2292\n",
            "Epoch: 041, Train loss: 1.1560, Test loss: 2.5738 Train Acc: 0.6134, Test Acc: 0.2393\n",
            "Epoch: 042, Train loss: 1.1209, Test loss: 2.6343 Train Acc: 0.6251, Test Acc: 0.2267\n",
            "Epoch: 043, Train loss: 1.1717, Test loss: 2.5813 Train Acc: 0.6080, Test Acc: 0.2413\n",
            "Epoch: 044, Train loss: 1.1271, Test loss: 2.6202 Train Acc: 0.6254, Test Acc: 0.2305\n",
            "Epoch: 045, Train loss: 1.1365, Test loss: 2.6120 Train Acc: 0.6221, Test Acc: 0.2358\n",
            "Epoch: 046, Train loss: 1.1111, Test loss: 2.5994 Train Acc: 0.6281, Test Acc: 0.2302\n",
            "Epoch: 047, Train loss: 1.1241, Test loss: 2.6187 Train Acc: 0.6258, Test Acc: 0.2310\n",
            "Epoch: 048, Train loss: 1.1300, Test loss: 2.5949 Train Acc: 0.6248, Test Acc: 0.2340\n",
            "Epoch: 049, Train loss: 1.1226, Test loss: 2.6193 Train Acc: 0.6260, Test Acc: 0.2307\n",
            "Epoch: 050, Train loss: 1.1379, Test loss: 2.5887 Train Acc: 0.6195, Test Acc: 0.2347\n",
            "Epoch: 051, Train loss: 1.1345, Test loss: 2.5599 Train Acc: 0.6213, Test Acc: 0.2385\n",
            "Epoch: 052, Train loss: 1.1346, Test loss: 2.5725 Train Acc: 0.6202, Test Acc: 0.2285\n",
            "Epoch: 053, Train loss: 1.1243, Test loss: 2.6128 Train Acc: 0.6245, Test Acc: 0.2325\n",
            "Epoch: 054, Train loss: 1.1209, Test loss: 2.5536 Train Acc: 0.6259, Test Acc: 0.2368\n",
            "Epoch: 055, Train loss: 1.1178, Test loss: 2.6012 Train Acc: 0.6274, Test Acc: 0.2307\n",
            "Epoch: 056, Train loss: 1.1115, Test loss: 2.5741 Train Acc: 0.6296, Test Acc: 0.2352\n",
            "Epoch: 057, Train loss: 1.1234, Test loss: 2.5800 Train Acc: 0.6250, Test Acc: 0.2380\n",
            "Epoch: 058, Train loss: 1.1257, Test loss: 2.5584 Train Acc: 0.6232, Test Acc: 0.2398\n",
            "Epoch: 059, Train loss: 1.1175, Test loss: 2.5979 Train Acc: 0.6262, Test Acc: 0.2352\n",
            "Epoch: 060, Train loss: 1.1243, Test loss: 2.6243 Train Acc: 0.6264, Test Acc: 0.2282\n",
            "Epoch: 061, Train loss: 1.1240, Test loss: 2.5826 Train Acc: 0.6251, Test Acc: 0.2423\n",
            "Epoch: 062, Train loss: 1.1166, Test loss: 2.6083 Train Acc: 0.6296, Test Acc: 0.2375\n",
            "Epoch: 063, Train loss: 1.1157, Test loss: 2.6167 Train Acc: 0.6267, Test Acc: 0.2300\n",
            "Epoch: 064, Train loss: 1.1166, Test loss: 2.6224 Train Acc: 0.6261, Test Acc: 0.2297\n",
            "Epoch: 065, Train loss: 1.1127, Test loss: 2.6153 Train Acc: 0.6277, Test Acc: 0.2330\n",
            "Epoch: 066, Train loss: 1.1123, Test loss: 2.5936 Train Acc: 0.6265, Test Acc: 0.2300\n",
            "Epoch: 067, Train loss: 1.1118, Test loss: 2.6108 Train Acc: 0.6290, Test Acc: 0.2272\n",
            "Epoch: 068, Train loss: 1.1094, Test loss: 2.6164 Train Acc: 0.6276, Test Acc: 0.2285\n",
            "Epoch: 069, Train loss: 1.1209, Test loss: 2.5807 Train Acc: 0.6263, Test Acc: 0.2350\n",
            "Epoch: 070, Train loss: 1.1205, Test loss: 2.6154 Train Acc: 0.6235, Test Acc: 0.2342\n",
            "Epoch: 071, Train loss: 1.1111, Test loss: 2.6169 Train Acc: 0.6292, Test Acc: 0.2290\n",
            "Epoch: 072, Train loss: 1.1108, Test loss: 2.6300 Train Acc: 0.6296, Test Acc: 0.2292\n",
            "Epoch: 073, Train loss: 1.1158, Test loss: 2.5609 Train Acc: 0.6286, Test Acc: 0.2380\n",
            "Epoch: 074, Train loss: 1.1034, Test loss: 2.5553 Train Acc: 0.6316, Test Acc: 0.2408\n",
            "Epoch: 075, Train loss: 1.1160, Test loss: 2.5993 Train Acc: 0.6264, Test Acc: 0.2330\n",
            "Epoch: 076, Train loss: 1.1231, Test loss: 2.6035 Train Acc: 0.6260, Test Acc: 0.2345\n",
            "Epoch: 077, Train loss: 1.1135, Test loss: 2.5692 Train Acc: 0.6302, Test Acc: 0.2363\n",
            "Epoch: 078, Train loss: 1.1190, Test loss: 2.5882 Train Acc: 0.6273, Test Acc: 0.2395\n",
            "Epoch: 079, Train loss: 1.1130, Test loss: 2.5645 Train Acc: 0.6292, Test Acc: 0.2363\n",
            "Epoch: 080, Train loss: 1.1066, Test loss: 2.6260 Train Acc: 0.6289, Test Acc: 0.2295\n",
            "Epoch: 081, Train loss: 1.1159, Test loss: 2.5560 Train Acc: 0.6267, Test Acc: 0.2393\n",
            "Epoch: 082, Train loss: 1.1102, Test loss: 2.5973 Train Acc: 0.6289, Test Acc: 0.2327\n",
            "Epoch: 083, Train loss: 1.1137, Test loss: 2.6196 Train Acc: 0.6272, Test Acc: 0.2267\n",
            "Epoch: 084, Train loss: 1.1086, Test loss: 2.6120 Train Acc: 0.6286, Test Acc: 0.2302\n",
            "Epoch: 085, Train loss: 1.1066, Test loss: 2.6578 Train Acc: 0.6299, Test Acc: 0.2252\n",
            "Epoch: 086, Train loss: 1.1142, Test loss: 2.5551 Train Acc: 0.6271, Test Acc: 0.2360\n",
            "Epoch: 087, Train loss: 1.1089, Test loss: 2.5793 Train Acc: 0.6293, Test Acc: 0.2393\n",
            "Epoch: 088, Train loss: 1.1078, Test loss: 2.5750 Train Acc: 0.6306, Test Acc: 0.2297\n",
            "Epoch: 089, Train loss: 1.1625, Test loss: 2.6013 Train Acc: 0.6106, Test Acc: 0.2350\n",
            "Epoch: 090, Train loss: 1.1300, Test loss: 2.5724 Train Acc: 0.6208, Test Acc: 0.2383\n",
            "Epoch: 091, Train loss: 1.1201, Test loss: 2.6157 Train Acc: 0.6267, Test Acc: 0.2305\n",
            "Epoch: 092, Train loss: 1.1090, Test loss: 2.6065 Train Acc: 0.6284, Test Acc: 0.2302\n",
            "Epoch: 093, Train loss: 1.1116, Test loss: 2.5837 Train Acc: 0.6276, Test Acc: 0.2380\n",
            "Epoch: 094, Train loss: 1.1188, Test loss: 2.6085 Train Acc: 0.6265, Test Acc: 0.2302\n",
            "Epoch: 095, Train loss: 1.1231, Test loss: 2.6242 Train Acc: 0.6273, Test Acc: 0.2275\n",
            "Epoch: 096, Train loss: 1.1214, Test loss: 2.6197 Train Acc: 0.6263, Test Acc: 0.2287\n",
            "Epoch: 097, Train loss: 1.1157, Test loss: 2.5963 Train Acc: 0.6251, Test Acc: 0.2358\n",
            "Epoch: 098, Train loss: 1.1226, Test loss: 2.6324 Train Acc: 0.6245, Test Acc: 0.2260\n",
            "Epoch: 099, Train loss: 1.1121, Test loss: 2.6441 Train Acc: 0.6295, Test Acc: 0.2325\n",
            "Epoch: 100, Train loss: 1.1136, Test loss: 2.6421 Train Acc: 0.6275, Test Acc: 0.2320\n",
            "Epoch: 101, Train loss: 1.1102, Test loss: 2.6561 Train Acc: 0.6285, Test Acc: 0.2330\n",
            "Epoch: 102, Train loss: 1.1120, Test loss: 2.6299 Train Acc: 0.6270, Test Acc: 0.2307\n",
            "Epoch: 103, Train loss: 1.1093, Test loss: 2.6243 Train Acc: 0.6302, Test Acc: 0.2300\n",
            "Epoch: 104, Train loss: 1.1081, Test loss: 2.6099 Train Acc: 0.6292, Test Acc: 0.2310\n",
            "Epoch: 105, Train loss: 1.1099, Test loss: 2.6114 Train Acc: 0.6271, Test Acc: 0.2325\n",
            "Epoch: 106, Train loss: 1.1136, Test loss: 2.6177 Train Acc: 0.6276, Test Acc: 0.2312\n",
            "Epoch: 107, Train loss: 1.1173, Test loss: 2.6432 Train Acc: 0.6256, Test Acc: 0.2275\n",
            "Epoch: 108, Train loss: 1.1206, Test loss: 2.5724 Train Acc: 0.6272, Test Acc: 0.2363\n",
            "Epoch: 109, Train loss: 1.1273, Test loss: 2.6105 Train Acc: 0.6273, Test Acc: 0.2312\n",
            "Epoch: 110, Train loss: 1.1251, Test loss: 2.6434 Train Acc: 0.6252, Test Acc: 0.2282\n",
            "Epoch: 111, Train loss: 1.1201, Test loss: 2.6332 Train Acc: 0.6254, Test Acc: 0.2312\n",
            "Epoch: 112, Train loss: 1.1107, Test loss: 2.6287 Train Acc: 0.6272, Test Acc: 0.2307\n",
            "Epoch: 113, Train loss: 1.1262, Test loss: 2.6716 Train Acc: 0.6238, Test Acc: 0.2224\n",
            "Epoch: 114, Train loss: 1.1379, Test loss: 2.6114 Train Acc: 0.6191, Test Acc: 0.2317\n",
            "Epoch: 115, Train loss: 1.1250, Test loss: 2.5980 Train Acc: 0.6251, Test Acc: 0.2368\n",
            "Epoch: 116, Train loss: 1.1073, Test loss: 2.6192 Train Acc: 0.6289, Test Acc: 0.2327\n",
            "Epoch: 117, Train loss: 1.1232, Test loss: 2.6362 Train Acc: 0.6244, Test Acc: 0.2355\n",
            "Epoch: 118, Train loss: 1.1253, Test loss: 2.6545 Train Acc: 0.6232, Test Acc: 0.2320\n",
            "Epoch: 119, Train loss: 1.1292, Test loss: 2.6782 Train Acc: 0.6207, Test Acc: 0.2267\n",
            "Epoch: 120, Train loss: 1.1250, Test loss: 2.6657 Train Acc: 0.6231, Test Acc: 0.2282\n",
            "Epoch: 121, Train loss: 1.1259, Test loss: 2.6539 Train Acc: 0.6222, Test Acc: 0.2325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg7aKj4j5P5V"
      },
      "source": [
        "## Fine tune with lower learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5HJTnpl2aeN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d43609a-d0e6-4f11-dae2-63f360809c35"
      },
      "source": [
        "PATH =  f\"{base_path}/model_first.bin\" \n",
        "SAVE_PATH = f\"{base_path}/model_fine1.bin\" \n",
        "#Change LR\n",
        "fine_tune(PATH, SAVE_PATH, log_interval=10, lr=.0004)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 060, Train loss: 1.1100, Test loss: 2.5161 Train Acc: 0.6156, Test Acc: 0.2295\n",
            "Epoch: 010, Train loss: 1.0657, Test loss: 2.4459 Train Acc: 0.6353, Test Acc: 0.2370\n",
            "Epoch: 020, Train loss: 1.0663, Test loss: 2.4610 Train Acc: 0.6346, Test Acc: 0.2360\n",
            "Epoch: 030, Train loss: 1.0667, Test loss: 2.4620 Train Acc: 0.6344, Test Acc: 0.2370\n",
            "Epoch: 040, Train loss: 1.0679, Test loss: 2.4659 Train Acc: 0.6336, Test Acc: 0.2380\n",
            "Epoch: 050, Train loss: 1.0677, Test loss: 2.4651 Train Acc: 0.6334, Test Acc: 0.2393\n",
            "Epoch: 060, Train loss: 1.0682, Test loss: 2.4666 Train Acc: 0.6333, Test Acc: 0.2398\n",
            "Epoch: 070, Train loss: 1.0666, Test loss: 2.4626 Train Acc: 0.6339, Test Acc: 0.2393\n",
            "Epoch: 080, Train loss: 1.0663, Test loss: 2.4605 Train Acc: 0.6341, Test Acc: 0.2425\n",
            "Epoch: 090, Train loss: 1.0660, Test loss: 2.4604 Train Acc: 0.6340, Test Acc: 0.2433\n",
            "Epoch: 100, Train loss: 1.0671, Test loss: 2.4606 Train Acc: 0.6337, Test Acc: 0.2430\n",
            "Epoch: 110, Train loss: 1.0671, Test loss: 2.4549 Train Acc: 0.6336, Test Acc: 0.2443\n",
            "Epoch: 120, Train loss: 1.0672, Test loss: 2.4524 Train Acc: 0.6333, Test Acc: 0.2455\n",
            "Epoch: 130, Train loss: 1.0677, Test loss: 2.4529 Train Acc: 0.6329, Test Acc: 0.2465\n",
            "Epoch: 140, Train loss: 1.0671, Test loss: 2.4493 Train Acc: 0.6331, Test Acc: 0.2486\n",
            "Epoch: 150, Train loss: 1.0671, Test loss: 2.4497 Train Acc: 0.6329, Test Acc: 0.2498\n",
            "Epoch: 160, Train loss: 1.0672, Test loss: 2.4505 Train Acc: 0.6326, Test Acc: 0.2473\n",
            "Epoch: 170, Train loss: 1.0668, Test loss: 2.4490 Train Acc: 0.6327, Test Acc: 0.2491\n",
            "Epoch: 180, Train loss: 1.0666, Test loss: 2.4519 Train Acc: 0.6332, Test Acc: 0.2493\n",
            "Epoch: 190, Train loss: 1.0666, Test loss: 2.4509 Train Acc: 0.6329, Test Acc: 0.2483\n",
            "Epoch: 200, Train loss: 1.0662, Test loss: 2.4519 Train Acc: 0.6334, Test Acc: 0.2493\n",
            "Epoch: 210, Train loss: 1.0658, Test loss: 2.4513 Train Acc: 0.6335, Test Acc: 0.2503\n",
            "Epoch: 220, Train loss: 1.0652, Test loss: 2.4513 Train Acc: 0.6336, Test Acc: 0.2506\n",
            "Epoch: 230, Train loss: 1.0645, Test loss: 2.4498 Train Acc: 0.6340, Test Acc: 0.2511\n",
            "Epoch: 240, Train loss: 1.0639, Test loss: 2.4487 Train Acc: 0.6343, Test Acc: 0.2498\n",
            "Epoch: 250, Train loss: 1.0625, Test loss: 2.4471 Train Acc: 0.6348, Test Acc: 0.2498\n",
            "Epoch: 260, Train loss: 1.0611, Test loss: 2.4443 Train Acc: 0.6352, Test Acc: 0.2511\n",
            "Epoch: 270, Train loss: 1.0607, Test loss: 2.4448 Train Acc: 0.6355, Test Acc: 0.2518\n",
            "Epoch: 280, Train loss: 1.0594, Test loss: 2.4402 Train Acc: 0.6359, Test Acc: 0.2528\n",
            "Epoch: 290, Train loss: 1.0587, Test loss: 2.4396 Train Acc: 0.6361, Test Acc: 0.2538\n",
            "Epoch: 300, Train loss: 1.0577, Test loss: 2.4386 Train Acc: 0.6365, Test Acc: 0.2541\n",
            "Epoch: 310, Train loss: 1.0568, Test loss: 2.4380 Train Acc: 0.6367, Test Acc: 0.2546\n",
            "Epoch: 320, Train loss: 1.0558, Test loss: 2.4365 Train Acc: 0.6370, Test Acc: 0.2551\n",
            "Epoch: 330, Train loss: 1.0557, Test loss: 2.4379 Train Acc: 0.6372, Test Acc: 0.2561\n",
            "Epoch: 340, Train loss: 1.0556, Test loss: 2.4392 Train Acc: 0.6371, Test Acc: 0.2566\n",
            "Epoch: 350, Train loss: 1.0552, Test loss: 2.4378 Train Acc: 0.6376, Test Acc: 0.2543\n",
            "Epoch: 360, Train loss: 1.0532, Test loss: 2.4319 Train Acc: 0.6380, Test Acc: 0.2553\n",
            "Epoch: 370, Train loss: 1.0525, Test loss: 2.4306 Train Acc: 0.6383, Test Acc: 0.2548\n",
            "Epoch: 380, Train loss: 1.0516, Test loss: 2.4287 Train Acc: 0.6387, Test Acc: 0.2561\n",
            "Epoch: 390, Train loss: 1.0504, Test loss: 2.4291 Train Acc: 0.6392, Test Acc: 0.2566\n",
            "Epoch: 400, Train loss: 1.0497, Test loss: 2.4266 Train Acc: 0.6393, Test Acc: 0.2561\n",
            "Epoch: 410, Train loss: 1.0487, Test loss: 2.4238 Train Acc: 0.6396, Test Acc: 0.2566\n",
            "Epoch: 420, Train loss: 1.0473, Test loss: 2.4190 Train Acc: 0.6398, Test Acc: 0.2573\n",
            "Epoch: 430, Train loss: 1.0482, Test loss: 2.4204 Train Acc: 0.6394, Test Acc: 0.2553\n",
            "Epoch: 440, Train loss: 1.0475, Test loss: 2.4178 Train Acc: 0.6396, Test Acc: 0.2563\n",
            "Epoch: 450, Train loss: 1.0472, Test loss: 2.4194 Train Acc: 0.6398, Test Acc: 0.2563\n",
            "Epoch: 460, Train loss: 1.0462, Test loss: 2.4145 Train Acc: 0.6397, Test Acc: 0.2558\n",
            "Epoch: 470, Train loss: 1.0450, Test loss: 2.4143 Train Acc: 0.6403, Test Acc: 0.2561\n",
            "Epoch: 480, Train loss: 1.0458, Test loss: 2.4167 Train Acc: 0.6399, Test Acc: 0.2563\n",
            "Epoch: 490, Train loss: 1.0446, Test loss: 2.4142 Train Acc: 0.6404, Test Acc: 0.2571\n",
            "Epoch: 500, Train loss: 1.0450, Test loss: 2.4171 Train Acc: 0.6407, Test Acc: 0.2563\n",
            "Epoch: 510, Train loss: 1.0445, Test loss: 2.4149 Train Acc: 0.6405, Test Acc: 0.2568\n",
            "Epoch: 520, Train loss: 1.0438, Test loss: 2.4140 Train Acc: 0.6406, Test Acc: 0.2568\n",
            "Epoch: 530, Train loss: 1.0437, Test loss: 2.4140 Train Acc: 0.6407, Test Acc: 0.2578\n",
            "Epoch: 540, Train loss: 1.0432, Test loss: 2.4129 Train Acc: 0.6409, Test Acc: 0.2576\n",
            "Epoch: 550, Train loss: 1.0435, Test loss: 2.4129 Train Acc: 0.6406, Test Acc: 0.2578\n",
            "Epoch: 560, Train loss: 1.0438, Test loss: 2.4123 Train Acc: 0.6405, Test Acc: 0.2581\n",
            "Epoch: 570, Train loss: 1.0427, Test loss: 2.4085 Train Acc: 0.6407, Test Acc: 0.2583\n",
            "Epoch: 580, Train loss: 1.0429, Test loss: 2.4109 Train Acc: 0.6410, Test Acc: 0.2581\n",
            "Epoch: 590, Train loss: 1.0423, Test loss: 2.4093 Train Acc: 0.6408, Test Acc: 0.2589\n",
            "Epoch: 600, Train loss: 1.0424, Test loss: 2.4088 Train Acc: 0.6409, Test Acc: 0.2571\n",
            "Epoch: 610, Train loss: 1.0418, Test loss: 2.4072 Train Acc: 0.6409, Test Acc: 0.2571\n",
            "Epoch: 620, Train loss: 1.0416, Test loss: 2.4065 Train Acc: 0.6409, Test Acc: 0.2573\n",
            "Epoch: 630, Train loss: 1.0417, Test loss: 2.4072 Train Acc: 0.6409, Test Acc: 0.2576\n",
            "Epoch: 640, Train loss: 1.0422, Test loss: 2.4053 Train Acc: 0.6406, Test Acc: 0.2583\n",
            "Epoch: 650, Train loss: 1.0417, Test loss: 2.4059 Train Acc: 0.6409, Test Acc: 0.2576\n",
            "Epoch: 660, Train loss: 1.0417, Test loss: 2.4050 Train Acc: 0.6408, Test Acc: 0.2576\n",
            "Epoch: 670, Train loss: 1.0415, Test loss: 2.4080 Train Acc: 0.6410, Test Acc: 0.2571\n",
            "Epoch: 680, Train loss: 1.0404, Test loss: 2.4054 Train Acc: 0.6414, Test Acc: 0.2583\n",
            "Epoch: 690, Train loss: 1.0406, Test loss: 2.4058 Train Acc: 0.6411, Test Acc: 0.2573\n",
            "Epoch: 700, Train loss: 1.0405, Test loss: 2.4068 Train Acc: 0.6412, Test Acc: 0.2576\n",
            "Epoch: 710, Train loss: 1.0396, Test loss: 2.4035 Train Acc: 0.6413, Test Acc: 0.2573\n",
            "Epoch: 720, Train loss: 1.0399, Test loss: 2.4066 Train Acc: 0.6416, Test Acc: 0.2576\n",
            "Epoch: 730, Train loss: 1.0392, Test loss: 2.4040 Train Acc: 0.6419, Test Acc: 0.2591\n",
            "Epoch: 740, Train loss: 1.0388, Test loss: 2.4042 Train Acc: 0.6421, Test Acc: 0.2599\n",
            "Epoch: 750, Train loss: 1.0385, Test loss: 2.4040 Train Acc: 0.6420, Test Acc: 0.2599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfgDZBdy7lJe"
      },
      "source": [
        "## Fine tune with still lower learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s0QBlHyF6-ac"
      },
      "source": [
        "PATH =  f\"{base_path}/model_fine1.bin\" \n",
        "SAVE_PATH = f\"{base_path}/model_fine2.bin\" \n",
        "#Change LR\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=.0001)\n",
        "fine_tune(PATH, SAVE_PATH, log_interval=10, lr=.0001)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}